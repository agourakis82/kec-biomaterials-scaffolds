"""DARWIN Knowledge Graph Builder - Sistema de Constru√ß√£o Autom√°tica

Sistema √©pico que constr√≥i automaticamente o Knowledge Graph interdisciplinar
conectando biomaterials, neuroscience, philosophy, quantum mechanics e psychiatry
atrav√©s de m√∫ltiplas fontes de dados integradas.
"""

import asyncio
import hashlib
import uuid
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from dataclasses import dataclass
import networkx as nx
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

from ..core.logging import get_logger
from ..models.knowledge_graph_models import (
    KnowledgeGraphTypes, ScientificDomains, NodeTypes, EdgeTypes,
    GraphNodeBase, PaperNode, ConceptNode, AuthorNode, MethodNode, InsightNode,
    GraphEdgeBase, CitationEdge, SimilarityEdge, BridgeEdge,
    KnowledgeGraphSnapshot, GraphStatsResponse
)

logger = get_logger("knowledge_graph.builder")


# ==================== INTERDISCIPLINARY BRIDGES CONFIGURATION ====================

INTERDISCIPLINARY_BRIDGES = {
    "biomaterials_neuroscience": {
        "neural_scaffolds", "bioelectronics", "neural_interfaces",
        "conductive_biomaterials", "neural_tissue_engineering",
        "biocompatible_electrodes", "neural_implants", "brain_machine_interface"
    },
    "neuroscience_philosophy": {
        "consciousness", "free_will", "mind_brain_problem", 
        "neural_correlates", "phenomenal_consciousness", "qualia",
        "intentionality", "cognitive_architecture", "mental_causation"
    },
    "philosophy_quantum": {
        "quantum_consciousness", "measurement_problem", "observer_effect",
        "quantum_mechanics_interpretation", "mind_matter_interaction",
        "quantum_information", "copenhagen_interpretation", "many_worlds"
    },
    "quantum_biomaterials": {
        "quantum_biology", "coherence_biomolecules", "quantum_effects_living",
        "photosynthesis_quantum", "enzyme_catalysis_quantum",
        "quantum_transport", "biological_quantum_computing"
    },
    "psychiatry_neuroscience": {
        "neural_circuits", "neurotransmitters", "brain_disorders",
        "psychiatric_biomarkers", "neuroplasticity", "synaptic_function",
        "neuroinflammation", "neurodevelopment"
    },
    "mathematics_all": {
        "graph_theory", "topology", "optimization", "algorithms",
        "statistical_analysis", "network_analysis", "spectral_analysis",
        "differential_equations", "linear_algebra", "probability_theory"
    }
}

# Keywords por dom√≠nio cient√≠fico
DOMAIN_KEYWORDS = {
    ScientificDomains.BIOMATERIALS: {
        "scaffold", "tissue_engineering", "biomaterial", "biocompatibility",
        "cell_adhesion", "mechanical_properties", "degradation", "porosity",
        "surface_modification", "bioactive_materials"
    },
    ScientificDomains.NEUROSCIENCE: {
        "neural_network", "synapse", "neuron", "brain", "cortex",
        "hippocampus", "plasticity", "action_potential", "neurotransmitter",
        "cognitive_function", "memory", "learning"
    },
    ScientificDomains.PHILOSOPHY: {
        "consciousness", "mind", "reality", "existence", "knowledge",
        "ethics", "logic", "metaphysics", "epistemology", "phenomenology",
        "ontology", "philosophy_of_mind"
    },
    ScientificDomains.QUANTUM_MECHANICS: {
        "quantum", "superposition", "entanglement", "wave_function",
        "measurement", "decoherence", "quantum_field", "particle",
        "uncertainty_principle", "quantum_information"
    },
    ScientificDomains.PSYCHIATRY: {
        "mental_health", "depression", "anxiety", "schizophrenia",
        "bipolar_disorder", "psychotherapy", "psychiatric_medication",
        "clinical_assessment", "psychological_intervention"
    },
    ScientificDomains.MATHEMATICS: {
        "theorem", "proof", "algorithm", "optimization", "statistics",
        "linear_algebra", "calculus", "topology", "graph_theory",
        "numerical_methods", "mathematical_modeling"
    }
}


@dataclass
class BuilderConfiguration:
    """Configura√ß√£o do Graph Builder."""
    include_citations: bool = True
    include_concepts: bool = True
    include_authors: bool = True
    include_methods: bool = True
    include_insights: bool = True
    similarity_threshold: float = 0.6
    citation_weight_factor: float = 1.0
    concept_weight_factor: float = 0.8
    bridge_weight_factor: float = 1.2
    max_nodes_per_build: int = 10000
    max_edges_per_build: int = 50000
    enable_incremental: bool = True
    cache_embeddings: bool = True


class KnowledgeGraphBuilder:
    """
    Sistema completo de constru√ß√£o autom√°tica do Knowledge Graph.
    
    Integra m√∫ltiplas fontes de dados:
    - RAG++ Database: Documentos cient√≠ficos indexados
    - Scientific Discovery: Papers novos descobertos via RSS  
    - Score Contracts: Resultados de an√°lises matem√°ticas
    - KEC Metrics: M√©tricas computadas de redes/scaffolds
    - Tree Search: Solu√ß√µes de otimiza√ß√£o descobertas
    - Multi-AI Hub: Insights de conversas com IAs
    """
    
    def __init__(self, config: Optional[BuilderConfiguration] = None):
        self.config = config or BuilderConfiguration()
        self.graph = nx.Graph()
        self.nodes_cache: Dict[str, GraphNodeBase] = {}
        self.edges_cache: Dict[str, GraphEdgeBase] = {}
        self.embeddings_cache: Dict[str, np.ndarray] = {}
        self.last_build: Optional[datetime] = None
        self.build_stats: Dict[str, Any] = {}
        
        # TF-IDF vectorizer para similaridade sem√¢ntica
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
        logger.info("üèóÔ∏è Knowledge Graph Builder initialized")
    
    # ==================== CORE BUILDING METHODS ====================
    
    async def build_complete_graph(
        self, 
        graph_types: List[KnowledgeGraphTypes],
        domains: Optional[List[ScientificDomains]] = None,
        force_rebuild: bool = False
    ) -> KnowledgeGraphSnapshot:
        """
        Constr√≥i o Knowledge Graph completo integrando todas as fontes.
        """
        logger.info(f"üöÄ Starting complete graph build - Types: {graph_types}")
        build_start = datetime.utcnow()
        
        try:
            # Clear ou usar cache existente
            if force_rebuild or not self.config.enable_incremental:
                await self._clear_caches()
                logger.info("üßπ Caches cleared for fresh build")
            
            # 1. Coletar dados de todas as fontes integradas
            logger.info("üìö Collecting data from integrated sources...")
            source_data = await self._collect_integrated_data(domains)
            
            # 2. Construir n√≥s baseado nos dados coletados
            logger.info("üîó Building nodes from collected data...")
            await self._build_nodes_from_sources(source_data)
            
            # 3. Construir arestas baseado nos tipos solicitados
            for graph_type in graph_types:
                logger.info(f"üîó Building edges for graph type: {graph_type}")
                await self._build_edges_by_type(graph_type, source_data)
            
            # 4. Aplicar algoritmos de linking interdisciplinar
            logger.info("üåê Applying interdisciplinary linking...")
            await self._apply_interdisciplinary_linking()
            
            # 5. Calcular m√©tricas e estat√≠sticas
            logger.info("üìä Computing graph statistics...")
            stats = await self._compute_graph_statistics()
            
            # 6. Criar snapshot final
            snapshot = await self._create_graph_snapshot(graph_types, stats)
            
            # Atualizar estado interno
            self.last_build = build_start
            self.build_stats = {
                "build_time": (datetime.utcnow() - build_start).total_seconds(),
                "graph_types": graph_types,
                "domains": domains,
                **stats
            }
            
            logger.info(f"‚úÖ Graph build completed in {self.build_stats['build_time']:.2f}s")
            logger.info(f"üìä Graph: {stats['total_nodes']} nodes, {stats['total_edges']} edges")
            
            return snapshot
            
        except Exception as e:
            logger.error(f"‚ùå Graph build failed: {e}")
            raise
    
    async def _collect_integrated_data(
        self, 
        domains: Optional[List[ScientificDomains]] = None
    ) -> Dict[str, Any]:
        """
        Coleta dados de todas as fontes integradas do DARWIN.
        """
        source_data = {
            "papers": [],
            "concepts": [],
            "authors": [],
            "methods": [],
            "insights": [],
            "citations": [],
            "kec_metrics": [],
            "tree_search_results": [],
            "multi_ai_conversations": []
        }
        
        # Coletar dados em paralelo
        collection_tasks = []
        
        # RAG++ Database
        collection_tasks.append(self._collect_rag_plus_data(domains))
        
        # Scientific Discovery
        collection_tasks.append(self._collect_scientific_discovery_data(domains))
        
        # Score Contracts
        collection_tasks.append(self._collect_score_contracts_data())
        
        # KEC Metrics
        collection_tasks.append(self._collect_kec_metrics_data())
        
        # Tree Search
        collection_tasks.append(self._collect_tree_search_data())
        
        # Multi-AI Hub
        collection_tasks.append(self._collect_multi_ai_data(domains))
        
        # Executar todas as coletas em paralelo
        results = await asyncio.gather(*collection_tasks, return_exceptions=True)
        
        # Consolidar resultados
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"Data collection task {i} failed: {result}")
                continue
            
            # Merge dos dados coletados
            if isinstance(result, dict):
                for key, value in result.items():
                    if key in source_data and isinstance(value, list):
                        source_data[key].extend(value)
        
        logger.info(f"üìö Data collection summary:")
        for key, items in source_data.items():
            if isinstance(items, list):
                logger.info(f"  - {key}: {len(items)} items")
        
        return source_data
    
    async def _collect_rag_plus_data(
        self, 
        domains: Optional[List[ScientificDomains]] = None
    ) -> Dict[str, List]:
        """Coleta dados do RAG++ Database."""
        try:
            # Simular coleta de papers indexados no RAG++
            # Em implementa√ß√£o real, integraria com o sistema RAG++
            papers = await self._simulate_rag_plus_papers(domains)
            concepts = await self._extract_concepts_from_papers(papers)
            citations = await self._extract_citations_from_papers(papers)
            
            return {
                "papers": papers,
                "concepts": concepts,
                "citations": citations
            }
        except Exception as e:
            logger.error(f"Failed to collect RAG++ data: {e}")
            return {"papers": [], "concepts": [], "citations": []}
    
    async def _collect_scientific_discovery_data(
        self, 
        domains: Optional[List[ScientificDomains]] = None
    ) -> Dict[str, List]:
        """Coleta dados do Scientific Discovery system."""
        try:
            # Simular papers descobertos via RSS monitoring
            discovered_papers = await self._simulate_discovered_papers(domains)
            novel_concepts = await self._extract_novel_concepts(discovered_papers)
            
            return {
                "papers": discovered_papers,
                "concepts": novel_concepts
            }
        except Exception as e:
            logger.error(f"Failed to collect Scientific Discovery data: {e}")
            return {"papers": [], "concepts": []}
    
    async def _collect_score_contracts_data(self) -> Dict[str, List]:
        """Coleta dados do Score Contracts system."""
        try:
            # Simular resultados de an√°lises matem√°ticas
            mathematical_methods = await self._simulate_mathematical_methods()
            mathematical_insights = await self._simulate_mathematical_insights()
            
            return {
                "methods": mathematical_methods,
                "insights": mathematical_insights
            }
        except Exception as e:
            logger.error(f"Failed to collect Score Contracts data: {e}")
            return {"methods": [], "insights": []}
    
    async def _collect_kec_metrics_data(self) -> Dict[str, List]:
        """Coleta dados do KEC Metrics system."""
        try:
            # Simular m√©tricas KEC computadas
            kec_insights = await self._simulate_kec_insights()
            network_methods = await self._simulate_network_methods()
            
            return {
                "insights": kec_insights,
                "methods": network_methods
            }
        except Exception as e:
            logger.error(f"Failed to collect KEC Metrics data: {e}")
            return {"insights": [], "methods": []}
    
    async def _collect_tree_search_data(self) -> Dict[str, List]:
        """Coleta dados do Tree Search PUCT system."""
        try:
            # Simular solu√ß√µes de otimiza√ß√£o descobertas
            optimization_insights = await self._simulate_optimization_insights()
            search_methods = await self._simulate_search_methods()
            
            return {
                "insights": optimization_insights,
                "methods": search_methods
            }
        except Exception as e:
            logger.error(f"Failed to collect Tree Search data: {e}")
            return {"insights": [], "methods": []}
    
    async def _collect_multi_ai_data(
        self, 
        domains: Optional[List[ScientificDomains]] = None
    ) -> Dict[str, List]:
        """Coleta dados do Multi-AI Hub."""
        try:
            # Simular insights de conversas com IAs
            ai_insights = await self._simulate_multi_ai_insights(domains)
            conversation_concepts = await self._extract_conversation_concepts(ai_insights)
            
            return {
                "insights": ai_insights,
                "concepts": conversation_concepts
            }
        except Exception as e:
            logger.error(f"Failed to collect Multi-AI data: {e}")
            return {"insights": [], "concepts": []}
    
    # ==================== NODE BUILDING METHODS ====================
    
    async def _build_nodes_from_sources(self, source_data: Dict[str, Any]):
        """Constr√≥i n√≥s do grafo baseado nos dados coletados."""
        
        # Construir n√≥s de papers
        if self.config.include_concepts and "papers" in source_data:
            for paper_data in source_data["papers"]:
                node = await self._create_paper_node(paper_data)
                await self._add_node_to_graph(node)
        
        # Construir n√≥s de conceitos
        if self.config.include_concepts and "concepts" in source_data:
            for concept_data in source_data["concepts"]:
                node = await self._create_concept_node(concept_data)
                await self._add_node_to_graph(node)
        
        # Construir n√≥s de autores
        if self.config.include_authors and "authors" in source_data:
            for author_data in source_data["authors"]:
                node = await self._create_author_node(author_data)
                await self._add_node_to_graph(node)
        
        # Construir n√≥s de m√©todos
        if self.config.include_methods and "methods" in source_data:
            for method_data in source_data["methods"]:
                node = await self._create_method_node(method_data)
                await self._add_node_to_graph(node)
        
        # Construir n√≥s de insights
        if self.config.include_insights and "insights" in source_data:
            for insight_data in source_data["insights"]:
                node = await self._create_insight_node(insight_data)
                await self._add_node_to_graph(node)
    
    async def _create_paper_node(self, paper_data: Dict[str, Any]) -> PaperNode:
        """Cria n√≥ de paper."""
        node_id = f"paper_{hashlib.md5(paper_data['title'].encode()).hexdigest()[:8]}"
        
        return PaperNode(
            id=node_id,
            label=paper_data["title"][:100] + "..." if len(paper_data["title"]) > 100 else paper_data["title"],
            domain=paper_data.get("domain", ScientificDomains.INTERDISCIPLINARY),
            title=paper_data["title"],
            authors=paper_data.get("authors", []),
            publication_date=paper_data.get("publication_date"),
            journal=paper_data.get("journal"),
            doi=paper_data.get("doi"),
            abstract=paper_data.get("abstract"),
            keywords=paper_data.get("keywords", []),
            citation_count=paper_data.get("citation_count", 0),
            properties={
                "source": paper_data.get("source", "unknown"),
                "confidence": paper_data.get("confidence", 0.8)
            }
        )
    
    async def _create_concept_node(self, concept_data: Dict[str, Any]) -> ConceptNode:
        """Cria n√≥ de conceito."""
        concept_text = concept_data.get("text", concept_data.get("name", "unknown_concept"))
        node_id = f"concept_{hashlib.md5(concept_text.encode()).hexdigest()[:8]}"
        
        return ConceptNode(
            id=node_id,
            label=concept_text,
            domain=concept_data.get("domain", ScientificDomains.INTERDISCIPLINARY),
            description=concept_data.get("description"),
            related_papers=concept_data.get("related_papers", []),
            frequency=concept_data.get("frequency", 1),
            properties={
                "extraction_method": concept_data.get("extraction_method", "unknown"),
                "confidence": concept_data.get("confidence", 0.7)
            }
        )
    
    async def _create_insight_node(self, insight_data: Dict[str, Any]) -> InsightNode:
        """Cria n√≥ de insight."""
        insight_text = insight_data.get("content", insight_data.get("text", ""))
        node_id = f"insight_{uuid.uuid4().hex[:12]}"
        
        return InsightNode(
            id=node_id,
            label=insight_text[:100] + "..." if len(insight_text) > 100 else insight_text,
            domain=insight_data.get("domain", ScientificDomains.INTERDISCIPLINARY),
            content=insight_text,
            confidence=insight_data.get("confidence", 0.6),
            source_system=insight_data.get("source", "unknown"),
            validation_status=insight_data.get("validation_status"),
            properties={
                "timestamp": insight_data.get("timestamp", datetime.utcnow().isoformat()),
                "domains_involved": insight_data.get("domains", [])
            }
        )
    
    # ==================== EDGE BUILDING METHODS ====================
    
    async def _build_edges_by_type(
        self, 
        graph_type: KnowledgeGraphTypes, 
        source_data: Dict[str, Any]
    ):
        """Constr√≥i arestas baseado no tipo de grafo."""
        
        if graph_type == KnowledgeGraphTypes.CITATION_NETWORK:
            await self._build_citation_edges(source_data.get("citations", []))
        
        elif graph_type == KnowledgeGraphTypes.CONCEPT_MAP:
            await self._build_concept_similarity_edges()
        
        elif graph_type == KnowledgeGraphTypes.METHODOLOGY_GRAPH:
            await self._build_methodology_edges()
        
        elif graph_type == KnowledgeGraphTypes.COLLABORATION_NETWORK:
            await self._build_collaboration_edges()
        
        elif graph_type == KnowledgeGraphTypes.TEMPORAL_GRAPH:
            await self._build_temporal_edges()
        
        elif graph_type == KnowledgeGraphTypes.INTERDISCIPLINARY:
            await self._build_interdisciplinary_edges()
    
    async def _build_citation_edges(self, citations: List[Dict[str, Any]]):
        """Constr√≥i arestas de cita√ß√£o."""
        for citation in citations:
            source_id = citation.get("source_paper")
            target_id = citation.get("target_paper")
            
            if source_id and target_id and source_id in self.nodes_cache and target_id in self.nodes_cache:
                edge = CitationEdge(
                    id=f"cite_{source_id}_{target_id}",
                    source=source_id,
                    target=target_id,
                    weight=self.config.citation_weight_factor,
                    citation_context=citation.get("context"),
                    citation_type=citation.get("type")
                )
                await self._add_edge_to_graph(edge)
    
    async def _build_concept_similarity_edges(self):
        """Constr√≥i arestas de similaridade entre conceitos."""
        concept_nodes = [node for node in self.nodes_cache.values() 
                        if isinstance(node, ConceptNode)]
        
        if len(concept_nodes) < 2:
            return
        
        # Calcular similaridade usando TF-IDF
        concept_texts = [f"{node.label} {node.description or ''}" for node in concept_nodes]
        
        try:
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(concept_texts)
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            for i, node_i in enumerate(concept_nodes):
                for j, node_j in enumerate(concept_nodes[i+1:], i+1):
                    similarity = similarity_matrix[i][j]
                    
                    if similarity > self.config.similarity_threshold:
                        edge = SimilarityEdge(
                            id=f"sim_{node_i.id}_{node_j.id}",
                            source=node_i.id,
                            target=node_j.id,
                            weight=similarity * self.config.concept_weight_factor,
                            similarity_score=similarity,
                            similarity_method="tfidf_cosine"
                        )
                        await self._add_edge_to_graph(edge)
                        
        except Exception as e:
            logger.warning(f"Failed to compute concept similarities: {e}")
    
    async def _apply_interdisciplinary_linking(self):
        """Aplica linking autom√°tico entre dom√≠nios."""
        for bridge_name, bridge_keywords in INTERDISCIPLINARY_BRIDGES.items():
            await self._create_bridge_connections(bridge_name, bridge_keywords)
    
    async def _create_bridge_connections(self, bridge_name: str, bridge_keywords: Set[str]):
        """Cria conex√µes ponte entre dom√≠nios."""
        # Encontrar n√≥s que cont√™m palavras-chave da ponte
        bridge_nodes = []
        
        for node in self.nodes_cache.values():
            node_text = f"{node.label} {getattr(node, 'description', '')}".lower()
            
            # Verificar se o n√≥ cont√©m keywords da ponte
            matching_keywords = [kw for kw in bridge_keywords 
                               if kw.replace('_', ' ') in node_text]
            
            if matching_keywords:
                bridge_nodes.append((node, matching_keywords))
        
        # Criar conex√µes entre n√≥s ponte
        for i, (node_i, keywords_i) in enumerate(bridge_nodes):
            for node_j, keywords_j in bridge_nodes[i+1:]:
                # Calcular for√ßa da ponte baseada em keywords compartilhadas
                shared_keywords = set(keywords_i) & set(keywords_j)
                bridge_strength = len(shared_keywords) / max(len(keywords_i), len(keywords_j))
                
                if bridge_strength > 0.2:  # Threshold para criar ponte
                    domains = self._extract_domains_from_bridge_name(bridge_name)
                    
                    edge = BridgeEdge(
                        id=f"bridge_{node_i.id}_{node_j.id}",
                        source=node_i.id,
                        target=node_j.id,
                        weight=bridge_strength * self.config.bridge_weight_factor,
                        domain_from=domains[0] if domains else ScientificDomains.INTERDISCIPLINARY,
                        domain_to=domains[1] if len(domains) > 1 else ScientificDomains.INTERDISCIPLINARY,
                        bridge_strength=bridge_strength,
                        properties={
                            "bridge_type": bridge_name,
                            "shared_keywords": list(shared_keywords)
                        }
                    )
                    await self._add_edge_to_graph(edge)
    
    # ==================== UTILITY METHODS ====================
    
    async def _add_node_to_graph(self, node: GraphNodeBase):
        """Adiciona n√≥ ao grafo e cache."""
        if len(self.nodes_cache) >= self.config.max_nodes_per_build:
            logger.warning(f"Reached max nodes limit: {self.config.max_nodes_per_build}")
            return
        
        self.nodes_cache[node.id] = node
        self.graph.add_node(node.id, **node.dict())
    
    async def _add_edge_to_graph(self, edge: GraphEdgeBase):
        """Adiciona aresta ao grafo e cache."""
        if len(self.edges_cache) >= self.config.max_edges_per_build:
            logger.warning(f"Reached max edges limit: {self.config.max_edges_per_build}")
            return
        
        # Verificar se os n√≥s existem
        if edge.source not in self.nodes_cache or edge.target not in self.nodes_cache:
            return
        
        self.edges_cache[edge.id] = edge
        self.graph.add_edge(edge.source, edge.target, **edge.dict())
    
    async def _compute_graph_statistics(self) -> Dict[str, Any]:
        """Computa estat√≠sticas do grafo."""
        try:
            nodes_by_type = defaultdict(int)
            nodes_by_domain = defaultdict(int)
            edges_by_type = defaultdict(int)
            
            for node in self.nodes_cache.values():
                nodes_by_type[node.type.value] += 1
                nodes_by_domain[node.domain.value] += 1
            
            for edge in self.edges_cache.values():
                edges_by_type[edge.type.value] += 1
            
            # Calcular m√©tricas de rede
            total_nodes = len(self.nodes_cache)
            total_edges = len(self.edges_cache)
            
            density = 0.0
            avg_degree = 0.0
            components = 0
            
            if total_nodes > 1:
                max_possible_edges = total_nodes * (total_nodes - 1) / 2
                density = total_edges / max_possible_edges if max_possible_edges > 0 else 0.0
                avg_degree = (2 * total_edges) / total_nodes if total_nodes > 0 else 0.0
                components = nx.number_connected_components(self.graph)
            
            # Contar conex√µes interdisciplinares
            interdisciplinary_connections = len([
                edge for edge in self.edges_cache.values()
                if isinstance(edge, BridgeEdge)
            ])
            
            return {
                "total_nodes": total_nodes,
                "total_edges": total_edges,
                "nodes_by_type": dict(nodes_by_type),
                "nodes_by_domain": dict(nodes_by_domain),
                "edges_by_type": dict(edges_by_type),
                "graph_density": density,
                "average_degree": avg_degree,
                "connected_components": components,
                "interdisciplinary_connections": interdisciplinary_connections
            }
            
        except Exception as e:
            logger.error(f"Failed to compute graph statistics: {e}")
            return {
                "total_nodes": len(self.nodes_cache),
                "total_edges": len(self.edges_cache),
                "nodes_by_type": {},
                "nodes_by_domain": {},
                "edges_by_type": {},
                "graph_density": 0.0,
                "average_degree": 0.0,
                "connected_components": 0,
                "interdisciplinary_connections": 0
            }
    
    async def _create_graph_snapshot(
        self, 
        graph_types: List[KnowledgeGraphTypes],
        stats: Dict[str, Any]
    ) -> KnowledgeGraphSnapshot:
        """Cria snapshot do grafo."""
        snapshot_id = f"kg_snapshot_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        return KnowledgeGraphSnapshot(
            id=snapshot_id,
            graph_type=graph_types[0] if graph_types else KnowledgeGraphTypes.INTERDISCIPLINARY,
            nodes=list(self.nodes_cache.values()),
            edges=list(self.edges_cache.values()),
            metadata={
                "build_timestamp": datetime.utcnow().isoformat(),
                "graph_types_included": [gt.value for gt in graph_types],
                "builder_config": self.config.__dict__,
                "statistics": stats
            }
        )
    
    async def _clear_caches(self):
        """Limpa todos os caches."""
        self.graph.clear()
        self.nodes_cache.clear()
        self.edges_cache.clear()
        self.embeddings_cache.clear()
    
    # ==================== SIMULATION METHODS (for MVP) ====================
    # Estes m√©todos simulam dados at√© que as integra√ß√µes reais sejam implementadas
    
    async def _simulate_rag_plus_papers(
        self, 
        domains: Optional[List[ScientificDomains]] = None
    ) -> List[Dict[str, Any]]:
        """Simula papers do RAG++ para demonstra√ß√£o."""
        sample_papers = [
            {
                "title": "Neural Scaffolds for Brain Tissue Engineering: A Comprehensive Review",
                "authors": ["Smith, J.", "Johnson, A.", "Brown, M."],
                "domain": ScientificDomains.BIOMATERIALS,
                "journal": "Biomaterials Science",
                "abstract": "This review examines the latest advances in neural scaffolds...",
                "keywords": ["neural scaffolds", "tissue engineering", "biomaterials"],
                "citation_count": 45,
                "source": "rag_plus"
            },
            {
                "title": "Quantum Effects in Biological Systems: From Photosynthesis to Consciousness",
                "authors": ["Wilson, K.", "Davis, L."],
                "domain": ScientificDomains.QUANTUM_MECHANICS,
                "journal": "Nature Physics",
                "abstract": "We explore quantum mechanical effects in biological systems...",
                "keywords": ["quantum biology", "consciousness", "photosynthesis"],
                "citation_count": 78,
                "source": "rag_plus"
            },
            {
                "title": "Philosophy of Mind and Neural Correlates of Consciousness",
                "authors": ["Taylor, R.", "Anderson, P."],
                "domain": ScientificDomains.PHILOSOPHY,
                "journal": "Journal of Philosophy",
                "abstract": "An examination of the relationship between mind and brain...",
                "keywords": ["consciousness", "philosophy of mind", "neural correlates"],
                "citation_count": 32,
                "source": "rag_plus"
            }
        ]
        
        # Filtrar por dom√≠nios se especificados
        if domains:
            sample_papers = [p for p in sample_papers if p["domain"] in domains]
        
        return sample_papers
    
    async def _extract_concepts_from_papers(
        self, 
        papers: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Extrai conceitos dos papers."""
        concepts = []
        
        for paper in papers:
            for keyword in paper.get("keywords", []):
                concepts.append({
                    "text": keyword,
                    "domain": paper["domain"],
                    "related_papers": [paper.get("id", paper["title"])],
                    "extraction_method": "keyword_extraction",
                    "confidence": 0.8
                })
        
        return concepts
    
    async def _simulate_multi_ai_insights(
        self, 
        domains: Optional[List[ScientificDomains]] = None
    ) -> List[Dict[str, Any]]:
        """Simula insights do Multi-AI Hub."""
        sample_insights = [
            {
                "content": "The intersection of quantum mechanics and consciousness suggests new approaches to neural interfaces",
                "domain": ScientificDomains.INTERDISCIPLINARY,
                "source": "multi_ai_chatgpt_claude",
                "confidence": 0.75,
                "domains": [ScientificDomains.QUANTUM_MECHANICS, ScientificDomains.NEUROSCIENCE, ScientificDomains.PHILOSOPHY]
            },
            {
                "content": "Biomaterial scaffolds with quantum properties could revolutionize tissue engineering",
                "domain": ScientificDomains.BIOMATERIALS,
                "source": "multi_ai_gemini",
                "confidence": 0.68,
                "domains": [ScientificDomains.BIOMATERIALS, ScientificDomains.QUANTUM_MECHANICS]
            }
        ]
        
        return sample_insights
    
    # Implementar outros m√©todos de simula√ß√£o...
    async def _simulate_discovered_papers(self, domains): return []
    async def _extract_novel_concepts(self, papers): return []
    async def _simulate_mathematical_methods(self): return []
    async def _simulate_mathematical_insights(self): return []
    async def _simulate_kec_insights(self): return []
    async def _simulate_network_methods(self): return []
    async def _simulate_optimization_insights(self): return []
    async def _simulate_search_methods(self): return []
    async def _extract_conversation_concepts(self, insights): return []
    async def _extract_citations_from_papers(self, papers): return []
    async def _create_author_node(self, author_data): pass
    async def _create_method_node(self, method_data): pass
    async def _build_methodology_edges(self): pass
    async def _build_collaboration_edges(self): pass
    async def _build_temporal_edges(self): pass
    async def _build_interdisciplinary_edges(self): pass
    
    def _extract_domains_from_bridge_name(self, bridge_name: str) -> List[ScientificDomains]:
        """Extrai dom√≠nios do nome da ponte."""
        domain_map = {
            "biomaterials": ScientificDomains.BIOMATERIALS,
            "neuroscience": ScientificDomains.NEUROSCIENCE,
            "philosophy": ScientificDomains.PHILOSOPHY,
            "quantum": ScientificDomains.QUANTUM_MECHANICS,
            "psychiatry": ScientificDomains.PSYCHIATRY,
            "mathematics": ScientificDomains.MATHEMATICS
        }
        
        domains = []
        for key, domain in domain_map.items():
            if key in bridge_name:
                domains.append(domain)
        
        return domains


__all__ = ["KnowledgeGraphBuilder", "BuilderConfiguration", "INTERDISCIPLINARY_BRIDGES"]