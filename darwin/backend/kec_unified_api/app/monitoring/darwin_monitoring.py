"""DARWIN Monitoring - Sistema √âpico de Observabilidade

üöÄ MONITORING REVOLUTIONARY - OBSERVABILIDADE BEYOND STATE-OF-THE-ART
Sistema completo de monitoring para o DARWIN AutoGen + JAX:

Features Disruptivas:
- üî• Real-time performance metrics (JAX speedup, throughput, etc.)
- üìä Structured logging with context √©pico
- üéØ Intelligent alerting system
- üí∞ Cost monitoring and optimization
- üè• Health checks automatizados
- üìà Performance dashboards
- üîç Error tracking and analysis
- üåä Distributed tracing support

Technology: Prometheus metrics + Structured logging + Alerting engine
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Callable, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import psutil
import threading
import queue

from ..core.logging import get_logger

logger = get_logger("darwin.monitoring")

# Colors for epic console output
class Colors:
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    PURPLE = '\033[0;35m'
    CYAN = '\033[0;36m'
    WHITE = '\033[1;37m'
    BOLD = '\033[1m'
    NC = '\033[0m'

@dataclass
class PerformanceMetric:
    """M√©trica de performance estruturada."""
    timestamp: datetime
    metric_name: str
    value: Union[int, float]
    unit: str
    labels: Dict[str, str]
    service: str = "darwin-unified-api"
    environment: str = "production"

@dataclass
class LogEntry:
    """Entrada de log estruturada."""
    timestamp: datetime
    level: str
    service: str
    operation: str
    message: str
    context: Dict[str, Any]
    trace_id: Optional[str] = None
    span_id: Optional[str] = None

@dataclass
class AlertCondition:
    """Condi√ß√£o de alerta inteligente."""
    name: str
    metric_name: str
    condition: str  # "gt", "lt", "eq", etc.
    threshold: Union[int, float]
    duration_seconds: int
    severity: str  # "info", "warning", "error", "critical"
    description: str
    cooldown_seconds: int = 300

@dataclass
class Alert:
    """Alerta ativo."""
    alert_id: str
    condition_name: str
    severity: str
    message: str
    triggered_at: datetime
    resolved_at: Optional[datetime] = None
    context: Dict[str, Any] = None

class DarwinMonitoring:
    """
    üöÄ DARWIN MONITORING ENGINE - OBSERVABILIDADE REVOLUTIONARY

    Engine √©pico de monitoring que fornece:
    - M√©tricas em tempo real de performance JAX
    - Logging estruturado com contexto completo
    - Sistema de alertas inteligente
    - Health checks automatizados
    - Cost monitoring
    - Performance dashboards
    """

    def __init__(self):
        self.monitoring_id = str(uuid.uuid4())
        self.is_initialized = False
        self.start_time = datetime.now(timezone.utc)

        # Metrics storage
        self.metrics_buffer: List[PerformanceMetric] = []
        self.metrics_queue = queue.Queue()

        # Logs storage
        self.logs_buffer: List[LogEntry] = []
        self.logs_queue = queue.Queue()

        # Alerting system
        self.alert_conditions: Dict[str, AlertCondition] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []

        # Performance tracking
        self.performance_stats = {
            "jax_operations": 0,
            "scaffold_processed": 0,
            "agent_interactions": 0,
            "total_processing_time_ms": 0.0,
            "average_jax_speedup": 0.0,
            "peak_memory_usage_mb": 0.0,
            "error_count": 0,
            "uptime_seconds": 0
        }

        # Health monitoring
        self.health_status = {
            "status": "healthy",
            "last_health_check": None,
            "cpu_usage_percent": 0.0,
            "memory_usage_percent": 0.0,
            "disk_usage_percent": 0.0,
            "services_status": {}
        }

        # Configuration
        self.config = {
            "metrics_buffer_size": 10000,
            "logs_buffer_size": 50000,
            "metrics_flush_interval": 60,  # seconds
            "logs_flush_interval": 30,     # seconds
            "health_check_interval": 60,   # seconds
            "alert_check_interval": 30,    # seconds
            "cost_monitoring_enabled": True,
            "performance_dashboard_enabled": True
        }

        # Background tasks
        self.background_tasks = []
        self.shutdown_event = threading.Event()

        logger.info(f"üöÄ DARWIN Monitoring Engine created: {self.monitoring_id}")

    async def initialize(self):
        """Inicializa o sistema de monitoring."""
        try:
            logger.info("üöÄ Inicializando DARWIN Monitoring Engine...")

            # Setup default alert conditions
            await self._setup_default_alerts()

            # Start background tasks
            await self._start_background_tasks()

            # Initial health check
            await self._perform_health_check()

            self.is_initialized = True
            logger.info("‚úÖ DARWIN Monitoring Engine initialized successfully")

        except Exception as e:
            logger.error(f"Falha na inicializa√ß√£o do monitoring: {e}")
            raise

    async def _setup_default_alerts(self):
        """Configura alertas padr√£o revolucion√°rios."""
        default_alerts = [
            AlertCondition(
                name="jax_speedup_degraded",
                metric_name="jax_speedup",
                condition="lt",
                threshold=10.0,
                duration_seconds=300,
                severity="warning",
                description="JAX speedup caiu abaixo de 10x - performance degradada"
            ),
            AlertCondition(
                name="high_error_rate",
                metric_name="error_rate",
                condition="gt",
                threshold=0.05,  # 5% error rate
                duration_seconds=300,
                severity="error",
                description="Taxa de erro acima de 5% - sistema inst√°vel"
            ),
            AlertCondition(
                name="memory_usage_critical",
                metric_name="memory_usage_percent",
                condition="gt",
                threshold=90.0,
                duration_seconds=60,
                severity="critical",
                description="Uso de mem√≥ria acima de 90% - risco de OOM"
            ),
            AlertCondition(
                name="scaffold_throughput_low",
                metric_name="scaffold_throughput",
                condition="lt",
                threshold=10.0,
                duration_seconds=600,
                severity="warning",
                description="Throughput de scaffolds abaixo de 10/s - performance baixa"
            )
        ]

        for alert in default_alerts:
            self.alert_conditions[alert.name] = alert

        logger.info(f"üéØ Configurados {len(default_alerts)} alertas inteligentes")

    async def _start_background_tasks(self):
        """Inicia tarefas em background."""
        tasks = [
            self._metrics_flush_worker(),
            self._logs_flush_worker(),
            self._health_check_worker(),
            self._alert_check_worker(),
            self._performance_aggregation_worker()
        ]

        for task in tasks:
            background_task = asyncio.create_task(task)
            self.background_tasks.append(background_task)

        logger.info(f"üîÑ Iniciadas {len(tasks)} tarefas de monitoring em background")

    # ===== METRICS COLLECTION =====

    async def record_metric(
        self,
        metric_name: str,
        value: Union[int, float],
        unit: str = "1",
        labels: Dict[str, str] = None,
        service: str = None
    ):
        """Registra uma m√©trica de performance."""
        if not self.is_initialized:
            return

        metric = PerformanceMetric(
            timestamp=datetime.now(timezone.utc),
            metric_name=metric_name,
            value=value,
            unit=unit,
            labels=labels or {},
            service=service or "darwin-unified-api"
        )

        # Add to buffer
        self.metrics_buffer.append(metric)

        # Update performance stats
        await self._update_performance_stats(metric)

        # Check buffer size
        if len(self.metrics_buffer) >= self.config["metrics_buffer_size"]:
            await self._flush_metrics()

    async def record_jax_performance(
        self,
        operation: str,
        matrix_size: int,
        processing_time_ms: float,
        speedup_factor: float,
        throughput_sps: float,
        memory_used_mb: float = 0.0
    ):
        """Registra m√©tricas espec√≠ficas de performance JAX."""
        labels = {
            "operation": operation,
            "matrix_size": str(matrix_size)
        }

        # Record multiple metrics
        await asyncio.gather(
            self.record_metric("jax_processing_time", processing_time_ms, "ms", labels),
            self.record_metric("jax_speedup", speedup_factor, "1", labels),
            self.record_metric("scaffold_throughput", throughput_sps, "1/s", labels),
            self.record_metric("memory_used", memory_used_mb, "MB", labels)
        )

        # Update counters
        self.performance_stats["jax_operations"] += 1
        self.performance_stats["scaffold_processed"] += max(1, int(throughput_sps * (processing_time_ms / 1000)))

    async def record_agent_interaction(
        self,
        agent_name: str,
        interaction_type: str,
        collaboration_score: float,
        response_quality: float,
        processing_time_ms: float
    ):
        """Registra m√©tricas de intera√ß√£o entre agentes."""
        labels = {
            "agent": agent_name,
            "interaction_type": interaction_type
        }

        await asyncio.gather(
            self.record_metric("agent_collaboration", collaboration_score, "1", labels),
            self.record_metric("response_quality", response_quality, "1", labels),
            self.record_metric("agent_processing_time", processing_time_ms, "ms", labels)
        )

        self.performance_stats["agent_interactions"] += 1

    async def _update_performance_stats(self, metric: PerformanceMetric):
        """Atualiza estat√≠sticas de performance."""
        if metric.metric_name == "jax_speedup":
            # Update rolling average
            current_avg = self.performance_stats["average_jax_speedup"]
            count = self.performance_stats["jax_operations"]
            if count > 0:
                self.performance_stats["average_jax_speedup"] = (current_avg * (count - 1) + metric.value) / count

        elif metric.metric_name == "memory_used":
            # Track peak memory
            if metric.value > self.performance_stats["peak_memory_usage_mb"]:
                self.performance_stats["peak_memory_usage_mb"] = metric.value

    # ===== LOGGING SYSTEM =====

    async def log_structured(
        self,
        level: str,
        operation: str,
        message: str,
        context: Dict[str, Any] = None,
        trace_id: str = None,
        span_id: str = None
    ):
        """Registra log estruturado com contexto √©pico."""
        if not self.is_initialized:
            return

        log_entry = LogEntry(
            timestamp=datetime.now(timezone.utc),
            level=level.upper(),
            service="darwin-unified-api",
            operation=operation,
            message=message,
            context=context or {},
            trace_id=trace_id,
            span_id=span_id
        )

        # Add to buffer
        self.logs_buffer.append(log_entry)

        # Update error count if error
        if level.upper() in ["ERROR", "CRITICAL"]:
            self.performance_stats["error_count"] += 1

        # Check buffer size
        if len(self.logs_buffer) >= self.config["logs_buffer_size"]:
            await self._flush_logs()

        # Also log to standard logger
        log_method = getattr(logger, level.lower(), logger.info)
        log_method(f"[{operation}] {message} | Context: {context}")

    async def log_performance_event(
        self,
        operation: str,
        scaffold_count: int,
        processing_time_ms: float,
        speedup_factor: float,
        throughput_sps: float,
        agent_collaboration_score: float = 0.0,
        research_quality_score: float = 0.0
    ):
        """Registra evento de performance √©pico."""
        context = {
            "scaffold_count": scaffold_count,
            "processing_time_ms": processing_time_ms,
            "speedup_factor": speedup_factor,
            "throughput_sps": throughput_sps,
            "agent_collaboration_score": agent_collaboration_score,
            "research_quality_score": research_quality_score,
            "performance_level": "revolutionary" if speedup_factor >= 100 else "achievement" if speedup_factor >= 10 else "baseline"
        }

        await self.log_structured(
            level="INFO",
            operation=operation,
            message=f"Performance √©pica: {speedup_factor:.1f}x speedup, {throughput_sps:.1f} scaffolds/s",
            context=context
        )

    # ===== ALERTING SYSTEM =====

    async def check_alerts(self):
        """Verifica condi√ß√µes de alerta."""
        if not self.is_initialized:
            return

        for condition in self.alert_conditions.values():
            await self._check_single_alert(condition)

    async def _check_single_alert(self, condition: AlertCondition):
        """Verifica uma condi√ß√£o espec√≠fica de alerta."""
        # Get current metric value
        current_value = await self._get_current_metric_value(condition.metric_name)
        
        if current_value is None:
            return

        # Check condition
        triggered = self._evaluate_condition(current_value, condition)
        
        # Handle alert state
        if triggered and condition.name not in self.active_alerts:
            await self._trigger_alert(condition, current_value)
        elif not triggered and condition.name in self.active_alerts:
            await self._resolve_alert(condition.name)

    def _evaluate_condition(self, value: float, condition: AlertCondition) -> bool:
        """Avalia se a condi√ß√£o foi atingida."""
        if condition.condition == "gt":
            return value > condition.threshold
        elif condition.condition == "lt":
            return value < condition.threshold
        elif condition.condition == "eq":
            return value == condition.threshold
        elif condition.condition == "gte":
            return value >= condition.threshold
        elif condition.condition == "lte":
            return value <= condition.threshold
        return False

    async def _trigger_alert(self, condition: AlertCondition, current_value: float):
        """Dispara um alerta."""
        alert = Alert(
            alert_id=str(uuid.uuid4()),
            condition_name=condition.name,
            severity=condition.severity,
            message=f"{condition.description} | Current value: {current_value}",
            triggered_at=datetime.now(timezone.utc),
            context={
                "metric_name": condition.metric_name,
                "current_value": current_value,
                "threshold": condition.threshold,
                "condition": condition.condition
            }
        )

        # Add to active alerts
        self.active_alerts[condition.name] = alert
        self.alert_history.append(alert)

        # Log alert
        await self.log_structured(
            level="ERROR" if condition.severity in ["error", "critical"] else "WARNING",
            operation="alerting",
            message=f"Alert triggered: {condition.name}",
            context=alert.context
        )

        # Print epic alert
        color = Colors.RED if condition.severity == "critical" else Colors.YELLOW
        print(f"{color}üö® [ALERT-{condition.severity.upper()}]{Colors.NC} {alert.message}")

    async def _resolve_alert(self, condition_name: str):
        """Resolve um alerta ativo."""
        if condition_name in self.active_alerts:
            alert = self.active_alerts[condition_name]
            alert.resolved_at = datetime.now(timezone.utc)
            
            # Remove from active alerts
            del self.active_alerts[condition_name]

            # Log resolution
            await self.log_structured(
                level="INFO",
                operation="alerting",
                message=f"Alert resolved: {condition_name}",
                context={"alert_id": alert.alert_id, "duration_seconds": (alert.resolved_at - alert.triggered_at).total_seconds()}
            )

            print(f"{Colors.GREEN}‚úÖ [ALERT-RESOLVED]{Colors.NC} {condition_name} resolved")

    async def _get_current_metric_value(self, metric_name: str) -> Optional[float]:
        """Obt√©m o valor atual de uma m√©trica."""
        # Get recent metrics
        recent_metrics = [m for m in self.metrics_buffer[-100:] if m.metric_name == metric_name]
        
        if not recent_metrics:
            # Check performance stats
            if metric_name in self.performance_stats:
                return self.performance_stats[metric_name]
            elif metric_name == "memory_usage_percent":
                return self.health_status["memory_usage_percent"]
            elif metric_name == "cpu_usage_percent":
                return self.health_status["cpu_usage_percent"]
            return None

        # Return most recent value
        return recent_metrics[-1].value

    # ===== HEALTH MONITORING =====

    async def _perform_health_check(self):
        """Executa health check abrangente."""
        try:
            # System metrics
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')

            self.health_status.update({
                "status": "healthy",
                "last_health_check": datetime.now(timezone.utc),
                "cpu_usage_percent": cpu_percent,
                "memory_usage_percent": memory.percent,
                "disk_usage_percent": disk.percent,
                "services_status": {
                    "darwin_api": "healthy",
                    "jax_engine": "healthy",
                    "monitoring": "healthy"
                }
            })

            # Record health metrics
            await asyncio.gather(
                self.record_metric("cpu_usage_percent", cpu_percent, "%"),
                self.record_metric("memory_usage_percent", memory.percent, "%"),
                self.record_metric("disk_usage_percent", disk.percent, "%")
            )

            # Check overall health
            if cpu_percent > 90 or memory.percent > 90 or disk.percent > 90:
                self.health_status["status"] = "degraded"
            
            # Log health status
            await self.log_structured(
                level="INFO",
                operation="health_check",
                message=f"Health check complete: {self.health_status['status']}",
                context={
                    "cpu_percent": cpu_percent,
                    "memory_percent": memory.percent,
                    "disk_percent": disk.percent
                }
            )

        except Exception as e:
            self.health_status["status"] = "unhealthy"
            logger.error(f"Health check failed: {e}")

    # ===== BACKGROUND WORKERS =====

    async def _metrics_flush_worker(self):
        """Worker para flush de m√©tricas."""
        while not self.shutdown_event.is_set():
            try:
                await asyncio.sleep(self.config["metrics_flush_interval"])
                if self.metrics_buffer:
                    await self._flush_metrics()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Metrics flush worker error: {e}")

    async def _logs_flush_worker(self):
        """Worker para flush de logs."""
        while not self.shutdown_event.is_set():
            try:
                await asyncio.sleep(self.config["logs_flush_interval"])
                if self.logs_buffer:
                    await self._flush_logs()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Logs flush worker error: {e}")

    async def _health_check_worker(self):
        """Worker para health checks."""
        while not self.shutdown_event.is_set():
            try:
                await self._perform_health_check()
                await asyncio.sleep(self.config["health_check_interval"])
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Health check worker error: {e}")

    async def _alert_check_worker(self):
        """Worker para verifica√ß√£o de alertas."""
        while not self.shutdown_event.is_set():
            try:
                await self.check_alerts()
                await asyncio.sleep(self.config["alert_check_interval"])
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Alert check worker error: {e}")

    async def _performance_aggregation_worker(self):
        """Worker para agrega√ß√£o de m√©tricas de performance."""
        while not self.shutdown_event.is_set():
            try:
                await asyncio.sleep(300)  # Every 5 minutes
                await self._aggregate_performance_stats()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Performance aggregation worker error: {e}")

    async def _flush_metrics(self):
        """Flush m√©tricas para persist√™ncia."""
        if not self.metrics_buffer:
            return

        try:
            # Convert to JSON for logging/storage
            metrics_data = [asdict(metric) for metric in self.metrics_buffer]
            
            # Log metrics (could be sent to external system)
            logger.info(f"üìä Flushing {len(metrics_data)} metrics to storage")
            
            # Save to file for local persistence
            metrics_file = Path("logs/darwin_metrics.jsonl")
            metrics_file.parent.mkdir(exist_ok=True)
            
            with open(metrics_file, "a") as f:
                for metric_data in metrics_data:
                    # Convert datetime to string
                    metric_data["timestamp"] = metric_data["timestamp"].isoformat()
                    f.write(json.dumps(metric_data) + "\n")

            # Clear buffer
            self.metrics_buffer.clear()

        except Exception as e:
            logger.error(f"Error flushing metrics: {e}")

    async def _flush_logs(self):
        """Flush logs para persist√™ncia."""
        if not self.logs_buffer:
            return

        try:
            # Convert to JSON for storage
            logs_data = [asdict(log_entry) for log_entry in self.logs_buffer]
            
            # Save to file
            logs_file = Path("logs/darwin_structured_logs.jsonl")
            logs_file.parent.mkdir(exist_ok=True)
            
            with open(logs_file, "a") as f:
                for log_data in logs_data:
                    # Convert datetime to string
                    log_data["timestamp"] = log_data["timestamp"].isoformat()
                    f.write(json.dumps(log_data) + "\n")

            # Clear buffer
            self.logs_buffer.clear()
            
            logger.info(f"üìù Flushed {len(logs_data)} structured logs")

        except Exception as e:
            logger.error(f"Error flushing logs: {e}")

    async def _aggregate_performance_stats(self):
        """Agrega estat√≠sticas de performance."""
        try:
            # Calculate uptime
            uptime = (datetime.now(timezone.utc) - self.start_time).total_seconds()
            self.performance_stats["uptime_seconds"] = uptime

            # Record aggregated metrics
            await asyncio.gather(
                self.record_metric("total_jax_operations", self.performance_stats["jax_operations"]),
                self.record_metric("total_scaffolds_processed", self.performance_stats["scaffold_processed"]),
                self.record_metric("average_jax_speedup", self.performance_stats["average_jax_speedup"]),
                self.record_metric("peak_memory_usage", self.performance_stats["peak_memory_usage_mb"], "MB"),
                self.record_metric("total_errors", self.performance_stats["error_count"]),
                self.record_metric("uptime", self.performance_stats["uptime_seconds"], "s")
            )

        except Exception as e:
            logger.error(f"Error aggregating performance stats: {e}")

    # ===== API ENDPOINTS =====

    async def get_metrics_summary(self) -> Dict[str, Any]:
        """Retorna resumo das m√©tricas."""
        return {
            "performance_stats": self.performance_stats.copy(),
            "health_status": self.health_status.copy(),
            "active_alerts": len(self.active_alerts),
            "total_alerts": len(self.alert_history),
            "metrics_buffer_size": len(self.metrics_buffer),
            "logs_buffer_size": len(self.logs_buffer),
            "monitoring_id": self.monitoring_id
        }

    async def get_recent_metrics(self, metric_name: str = None, limit: int = 100) -> List[Dict[str, Any]]:
        """Retorna m√©tricas recentes."""
        metrics = self.metrics_buffer[-limit:]
        
        if metric_name:
            metrics = [m for m in metrics if m.metric_name == metric_name]
            
        return [asdict(m) for m in metrics]

    async def get_recent_logs(self, level: str = None, limit: int = 100) -> List[Dict[str, Any]]:
        """Retorna logs recentes."""
        logs = self.logs_buffer[-limit:]
        
        if level:
            logs = [l for l in logs if l.level == level.upper()]
            
        return [asdict(l) for l in logs]

    async def get_active_alerts(self) -> List[Dict[str, Any]]:
        """Retorna alertas ativos."""
        return [asdict(alert) for alert in self.active_alerts.values()]

    # ===== DASHBOARD DATA =====

    async def get_dashboard_data(self) -> Dict[str, Any]:
        """Retorna dados √©picos para dashboard."""
        # Recent performance metrics
        recent_metrics = self.metrics_buffer[-50:]
        
        # Group metrics by name
        metrics_by_name = {}
        for metric in recent_metrics:
            if metric.metric_name not in metrics_by_name:
                metrics_by_name[metric.metric_name] = []
            metrics_by_name[metric.metric_name].append({
                "timestamp": metric.timestamp.isoformat(),
                "value": metric.value,
                "labels": metric.labels
            })

        # Performance summary
        performance_level = "revolutionary" if self.performance_stats["average_jax_speedup"] >= 100 else "achievement" if self.performance_stats["average_jax_speedup"] >= 10 else "baseline"

        return {
            "overview": {
                "status": self.health_status["status"],
                "performance_level": performance_level,
                "active_alerts": len(self.active_alerts),
                "jax_operations": self.performance_stats["jax_operations"],
                "scaffolds_processed": self.performance_stats["scaffold_processed"],
                "average_speedup": self.performance_stats["average_jax_speedup"],
                "uptime_seconds": self.performance_stats["uptime_seconds"]
            },
            "metrics": metrics_by_name,
            "health": self.health_status,
            "alerts": [asdict(alert) for alert in list(self.active_alerts.values())[-10:]],
            "recent_logs": [asdict(log) for log in self.logs_buffer[-20:]]
        }

    # ===== SHUTDOWN =====

    async def shutdown(self):
        """Shutdown graceful do sistema de monitoring."""
        try:
            logger.info("üõë Shutting down DARWIN Monitoring Engine...")

            # Set shutdown event
            self.shutdown_event.set()

            # Cancel background tasks
            for task in self.background_tasks:
                if not task.done():
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass

            # Final flush
            if self.metrics_buffer:
                await self._flush_metrics()
            if self.logs_buffer:
                await self._flush_logs()

            # Final stats log
            await self.log_structured(
                level="INFO",
                operation="shutdown",
                message="DARWIN Monitoring Engine shutdown complete",
                context={
                    "total_metrics": self.performance_stats["jax_operations"],
                    "total_scaffolds": self.performance_stats["scaffold_processed"],
                    "total_alerts": len(self.alert_history),
                    "uptime_seconds": self.performance_stats["uptime_seconds"]
                }
            )

            self.is_initialized = False
            logger.info("‚úÖ DARWIN Monitoring Engine shutdown complete")

        except Exception as e:
            logger.error(f"Error during monitoring shutdown: {e}")


# ==================== GLOBAL MONITORING INSTANCE ====================

# Global monitoring instance
_global_monitoring: Optional[DarwinMonitoring] = None

async def get_monitoring() -> DarwinMonitoring:
    """Obt√©m a inst√¢ncia global de monitoring."""
    global _global_monitoring
    
    if _global_monitoring is None:
        _global_monitoring = DarwinMonitoring()
        await _global_monitoring.initialize()
    
    return _global_monitoring

async def record_performance(
    operation: str,
    matrix_size: int = 0,
    processing_time_ms: float = 0.0,
    speedup_factor: float = 1.0,
    throughput_sps: float = 0.0,
    memory_used_mb: float = 0.0
):
    """Fun√ß√£o helper para registrar performance."""
    monitoring = await get_monitoring()
    
    if matrix_size > 0:
        await monitoring.record_jax_performance(
            operation=operation,
            matrix_size=matrix_size,
            processing_time_ms=processing_time_ms,
            speedup_factor=speedup_factor,
            throughput_sps=throughput_sps,
            memory_used_mb=memory_used_mb
        )

async def log_epic_event(
    operation: str,
    message: str,
    level: str = "INFO",
    context: Dict[str, Any] = None
):
    """Fun√ß√£o helper para logging √©pico."""
    monitoring = await get_monitoring()
    await monitoring.log_structured(
        level=level,
        operation=operation,
        message=message,
        context=context
    )


# ==================== EXPORTS ====================

__all__ = [
    "DarwinMonitoring",
    "PerformanceMetric",
    "LogEntry",
    "AlertCondition",
    "Alert",
    "get_monitoring",
    "record_performance",
    "log_epic_event"
]