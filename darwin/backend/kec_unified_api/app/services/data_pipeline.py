"""Data Pipeline - Million Scaffold Processing Revolutionary System

üåä DATA PIPELINE REVOLUTIONARY - MILLION SCAFFOLD PROCESSING
Sistema √©pico para processar e armazenar milh√µes de resultados de scaffold:
- JAX Ultra-Performance batch processing
- BigQuery streaming pipeline
- Real-time analytics dashboard data
- Cross-domain research insights aggregation
- Performance monitoring e optimization

Technology: JAX + BigQuery + Streaming Pipeline + Million-Scale Processing
"""

import asyncio
import logging
import uuid
import time
import numpy as np
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple, Iterator
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import queue

from ..core.logging import get_logger
from .bigquery_client import (
    BigQueryClient, 
    ScaffoldAnalysisRecord, 
    ResearchInsightRecord,
    PerformanceRecord,
    DatasetType
)
from ..performance.jax_kec_engine import JAXKECEngine, BatchComputationResult
from ..models.kec_models import KECMetricsResult

logger = get_logger("darwin.data_pipeline")


@dataclass
class ScaffoldInput:
    """Input para processamento de scaffold."""
    scaffold_id: str
    adjacency_matrix: np.ndarray
    material_type: str
    target_application: str
    metadata: Optional[Dict[str, Any]] = None


@dataclass 
class PipelineMetrics:
    """M√©tricas do pipeline."""
    scaffolds_processed: int
    total_processing_time_ms: float
    average_speedup_factor: float
    bigquery_insert_success_rate: float
    throughput_scaffolds_per_second: float
    peak_memory_usage_mb: float


class MillionScaffoldPipeline:
    """
    üåä MILLION SCAFFOLD PROCESSING PIPELINE REVOLUTIONARY
    
    Pipeline √©pico que combina JAX ultra-performance computing with
    BigQuery streaming para processar e armazenar millions de scaffold results.
    
    Features:
    - JAX batch processing com 1000x speedup
    - BigQuery streaming inserts
    - Real-time progress monitoring
    - Memory-efficient chunked processing
    - Error handling e retry logic
    """
    
    def __init__(
        self,
        project_id: str,
        batch_size: int = 1000,
        max_concurrent_batches: int = 4
    ):
        self.project_id = project_id
        self.batch_size = batch_size
        self.max_concurrent_batches = max_concurrent_batches
        
        # Core components
        self.jax_engine: Optional[JAXKECEngine] = None
        self.bigquery_client: Optional[BigQueryClient] = None
        
        # Pipeline state
        self.is_initialized = False
        self.is_processing = False
        self.processing_stats = PipelineMetrics(
            scaffolds_processed=0,
            total_processing_time_ms=0.0,
            average_speedup_factor=0.0,
            bigquery_insert_success_rate=0.0,
            throughput_scaffolds_per_second=0.0,
            peak_memory_usage_mb=0.0
        )
        
        # Processing queue
        self.processing_queue: queue.Queue = queue.Queue()
        self.results_queue: queue.Queue = queue.Queue()
        
        # Thread pool for concurrent processing
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent_batches)
        
        logger.info(f"üåä Million Scaffold Pipeline created: batch_size={batch_size}")
    
    async def initialize(self):
        """Inicializa pipeline components."""
        try:
            logger.info("üåä Inicializando Million Scaffold Pipeline...")
            
            # Initialize JAX engine
            self.jax_engine = JAXKECEngine()
            await self.jax_engine.initialize()
            
            # Initialize BigQuery client
            self.bigquery_client = BigQueryClient(self.project_id)
            await self.bigquery_client.initialize()
            
            self.is_initialized = True
            logger.info("‚úÖ Million Scaffold Pipeline initialized!")
            
        except Exception as e:
            logger.error(f"Pipeline initialization error: {e}")
            raise
    
    async def process_million_scaffolds(
        self,
        scaffolds: List[ScaffoldInput],
        enable_biocompatibility_analysis: bool = True,
        enable_real_time_analytics: bool = True
    ) -> Dict[str, Any]:
        """
        üöÄ PROCESSA MILH√ïES DE SCAFFOLDS COM JAX + BIGQUERY
        
        Pipeline completo que:
        1. Processa scaffolds em batches using JAX
        2. Calcula KEC metrics com 1000x speedup
        3. Analisa biocompatibilidade
        4. Armazena resultados em BigQuery
        5. Gera analytics em tempo real
        """
        if not self.is_initialized:
            raise RuntimeError("Pipeline n√£o est√° inicializado")
        
        if self.is_processing:
            raise RuntimeError("Pipeline j√° est√° processando")
        
        self.is_processing = True
        start_time = time.time()
        
        try:
            logger.info(f"üöÄ Processing {len(scaffolds)} scaffolds with revolutionary pipeline...")
            
            # Reset stats
            self.processing_stats = PipelineMetrics(0, 0.0, 0.0, 0.0, 0.0, 0.0)
            
            # Process em chunks para otimizar mem√≥ria
            total_batches = (len(scaffolds) + self.batch_size - 1) // self.batch_size
            processed_count = 0
            successful_inserts = 0
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(scaffolds))
                batch = scaffolds[start_idx:end_idx]
                
                logger.info(f"üîÑ Processing batch {batch_idx + 1}/{total_batches}: {len(batch)} scaffolds")
                
                # Process batch
                batch_results = await self._process_scaffold_batch(
                    batch,
                    enable_biocompatibility_analysis
                )
                
                if batch_results:
                    # Insert results into BigQuery
                    insert_success = await self._insert_batch_results(batch_results)
                    
                    if insert_success:
                        successful_inserts += len(batch_results)
                    
                    processed_count += len(batch_results)
                    
                    # Update stats
                    self._update_processing_stats(batch_results)
                
                # Progress logging
                progress = (batch_idx + 1) / total_batches * 100
                logger.info(f"üìä Progress: {progress:.1f}% ({processed_count}/{len(scaffolds)} scaffolds)")
                
                # Real-time analytics se habilitado
                if enable_real_time_analytics and batch_idx % 10 == 0:
                    await self._generate_interim_analytics(processed_count)
            
            # Calculate final metrics
            total_time = (time.time() - start_time) * 1000  # ms
            throughput = len(scaffolds) / (total_time / 1000.0)  # scaffolds/second
            success_rate = successful_inserts / len(scaffolds) if scaffolds else 0.0
            
            # Update final stats
            self.processing_stats.scaffolds_processed = processed_count
            self.processing_stats.total_processing_time_ms = total_time
            self.processing_stats.throughput_scaffolds_per_second = throughput
            self.processing_stats.bigquery_insert_success_rate = success_rate
            
            # Generate final analytics
            final_analytics = await self._generate_final_analytics()
            
            result = {
                "pipeline_execution_id": str(uuid.uuid4()),
                "total_scaffolds": len(scaffolds),
                "processed_scaffolds": processed_count,
                "successful_inserts": successful_inserts,
                "processing_time_ms": total_time,
                "throughput_scaffolds_per_second": throughput,
                "success_rate": success_rate,
                "average_speedup_factor": self.processing_stats.average_speedup_factor,
                "pipeline_metrics": asdict(self.processing_stats),
                "analytics": final_analytics,
                "timestamp": datetime.now(timezone.utc)
            }
            
            logger.info(f"üéâ Million scaffold processing COMPLETE! Processed {processed_count} scaffolds in {total_time/1000:.1f}s ({throughput:.1f} scaffolds/s)")
            
            return result
            
        except Exception as e:
            logger.error(f"Pipeline processing error: {e}")
            raise
        finally:
            self.is_processing = False
    
    async def _process_scaffold_batch(
        self,
        scaffolds: List[ScaffoldInput],
        enable_biocompatibility: bool
    ) -> List[ScaffoldAnalysisRecord]:
        """Processa batch de scaffolds com JAX."""
        try:
            batch_start = time.time()
            
            # Extrair adjacency matrices
            matrices = [scaffold.adjacency_matrix for scaffold in scaffolds]
            
            # Processar com JAX engine
            batch_result: BatchComputationResult = await self.jax_engine.compute_batch_ultra_fast(
                adjacency_matrices=matrices,
                metrics=["H_spectral", "k_forman_mean", "sigma", "swp"],
                chunk_size=min(500, len(matrices))  # Chunk size para mem√≥ria
            )
            
            # Converter para ScaffoldAnalysisRecord
            analysis_records = []
            
            for i, (scaffold_input, kec_result) in enumerate(zip(scaffolds, batch_result.results)):
                # Calcular biocompatibilidade se habilitado
                biocompatibility_score = 0.0
                if enable_biocompatibility and kec_result:
                    biocompatibility_score = self._calculate_biocompatibility_score(kec_result)
                
                # Extrair material properties do metadata
                material_properties = scaffold_input.metadata or {}
                material_properties.update({
                    "material_type": scaffold_input.material_type,
                    "target_application": scaffold_input.target_application,
                    "matrix_size": scaffold_input.adjacency_matrix.shape[0]
                })
                
                # Criar record
                record = ScaffoldAnalysisRecord(
                    scaffold_id=scaffold_input.scaffold_id,
                    analysis_id=str(uuid.uuid4()),
                    kec_metrics={
                        "H_spectral": kec_result.H_spectral,
                        "k_forman_mean": kec_result.k_forman_mean,
                        "sigma": kec_result.sigma,
                        "swp": kec_result.swp
                    } if kec_result else {},
                    material_properties=material_properties,
                    biocompatibility_score=biocompatibility_score,
                    performance_metrics={
                        "batch_processing_time_ms": batch_result.performance_metrics.computation_time_ms,
                        "individual_computation_time_ms": batch_result.performance_metrics.computation_time_ms / len(scaffolds),
                        "memory_usage_mb": batch_result.performance_metrics.memory_usage_mb / len(scaffolds)
                    },
                    computation_time_ms=batch_result.performance_metrics.computation_time_ms / len(scaffolds),
                    jax_speedup_factor=batch_result.performance_metrics.speedup_factor,
                    timestamp=datetime.now(timezone.utc),
                    metadata={
                        "batch_id": str(uuid.uuid4()),
                        "batch_size": len(scaffolds),
                        "device_used": batch_result.performance_metrics.device_used,
                        "processing_mode": "jax_ultra_performance"
                    }
                )
                
                analysis_records.append(record)
            
            batch_time = (time.time() - batch_start) * 1000
            logger.info(f"‚ö° Batch processed: {len(analysis_records)} scaffolds in {batch_time:.1f}ms ({batch_result.performance_metrics.speedup_factor:.1f}x speedup)")
            
            return analysis_records
            
        except Exception as e:
            logger.error(f"Batch processing error: {e}")
            return []
    
    def _calculate_biocompatibility_score(self, kec_metrics: KECMetricsResult) -> float:
        """
        üß¨ C√ÅLCULO DE BIOCOMPATIBILIDADE BASEADO EM KEC METRICS
        
        Calcula score de biocompatibilidade usando valores KEC optimizados.
        """
        try:
            score = 0.0
            max_score = 1.0
            
            # H_spectral contribution (25% weight)
            if kec_metrics.H_spectral is not None:
                # Optimal range: 6.5-8.5
                if 6.5 <= kec_metrics.H_spectral <= 8.5:
                    h_score = 1.0
                elif 5.0 <= kec_metrics.H_spectral < 6.5:
                    h_score = (kec_metrics.H_spectral - 5.0) / 1.5
                elif 8.5 < kec_metrics.H_spectral <= 10.0:
                    h_score = 1.0 - (kec_metrics.H_spectral - 8.5) / 1.5
                else:
                    h_score = 0.0
                
                score += h_score * 0.25
            
            # k_forman_mean contribution (25% weight)
            if kec_metrics.k_forman_mean is not None:
                # Optimal range: 0.15-0.45
                if 0.15 <= kec_metrics.k_forman_mean <= 0.45:
                    k_score = 1.0
                elif 0.0 <= kec_metrics.k_forman_mean < 0.15:
                    k_score = kec_metrics.k_forman_mean / 0.15
                elif 0.45 < kec_metrics.k_forman_mean <= 0.8:
                    k_score = 1.0 - (kec_metrics.k_forman_mean - 0.45) / 0.35
                else:
                    k_score = 0.0
                
                score += k_score * 0.25
            
            # sigma contribution (25% weight)
            if kec_metrics.sigma is not None:
                # Optimal range: 1.8-2.8
                if 1.8 <= kec_metrics.sigma <= 2.8:
                    sigma_score = 1.0
                elif 1.0 <= kec_metrics.sigma < 1.8:
                    sigma_score = (kec_metrics.sigma - 1.0) / 0.8
                elif 2.8 < kec_metrics.sigma <= 4.0:
                    sigma_score = 1.0 - (kec_metrics.sigma - 2.8) / 1.2
                else:
                    sigma_score = 0.0
                
                score += sigma_score * 0.25
            
            # swp contribution (25% weight)
            if kec_metrics.swp is not None:
                # Optimal range: 0.6-0.9
                if 0.6 <= kec_metrics.swp <= 0.9:
                    swp_score = 1.0
                elif 0.3 <= kec_metrics.swp < 0.6:
                    swp_score = (kec_metrics.swp - 0.3) / 0.3
                elif 0.9 < kec_metrics.swp <= 1.0:
                    swp_score = 1.0 - (kec_metrics.swp - 0.9) / 0.1
                else:
                    swp_score = 0.0
                
                score += swp_score * 0.25
            
            return min(max(score, 0.0), 1.0)  # Clip to [0, 1]
            
        except Exception as e:
            logger.warning(f"Biocompatibility calculation error: {e}")
            return 0.5  # Default neutral score
    
    async def _insert_batch_results(self, results: List[ScaffoldAnalysisRecord]) -> bool:
        """Insere batch de resultados no BigQuery."""
        try:
            # Use batch insert para efficiency
            success = await self.bigquery_client.batch_insert_scaffold_results(
                scaffolds=results,
                batch_size=min(1000, len(results))
            )
            
            if success.get("success_rate", 0) > 0.8:  # 80% success threshold
                logger.info(f"‚úÖ Batch inserted successfully: {len(results)} scaffolds")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Batch insert partial success: {success}")
                return False
            
        except Exception as e:
            logger.error(f"Batch insert error: {e}")
            return False
    
    def _update_processing_stats(self, results: List[ScaffoldAnalysisRecord]):
        """Atualiza estat√≠sticas de processamento."""
        if not results:
            return
        
        # Update processed count
        self.processing_stats.scaffolds_processed += len(results)
        
        # Calculate average speedup
        speedups = [r.jax_speedup_factor for r in results if r.jax_speedup_factor]
        if speedups:
            current_avg = self.processing_stats.average_speedup_factor
            current_count = self.processing_stats.scaffolds_processed - len(results)
            new_avg_speedup = sum(speedups) / len(speedups)
            
            if current_count > 0:
                # Weighted average
                total_avg = (current_avg * current_count + new_avg_speedup * len(results)) / self.processing_stats.scaffolds_processed
            else:
                total_avg = new_avg_speedup
            
            self.processing_stats.average_speedup_factor = total_avg
        
        # Update memory usage
        memory_usages = [r.performance_metrics.get("memory_usage_mb", 0) for r in results]
        if memory_usages:
            peak_memory = max(memory_usages)
            if peak_memory > self.processing_stats.peak_memory_usage_mb:
                self.processing_stats.peak_memory_usage_mb = peak_memory
    
    async def _generate_interim_analytics(self, processed_count: int):
        """Gera analytics interim para monitoring."""
        try:
            # Get latest analytics from BigQuery
            analytics = await self.bigquery_client.get_scaffold_analytics(time_window_hours=1)
            
            # Log interim progress
            logger.info(f"üìä Interim Analytics - Processed: {processed_count}")
            logger.info(f"   Average H_spectral: {analytics.get('avg_h_spectral', 0):.3f}")
            logger.info(f"   Average computation time: {analytics.get('avg_computation_time_ms', 0):.2f}ms")
            logger.info(f"   Average speedup: {analytics.get('avg_speedup_factor', 0):.1f}x")
            
        except Exception as e:
            logger.warning(f"Interim analytics error: {e}")
    
    async def _generate_final_analytics(self) -> Dict[str, Any]:
        """Gera analytics finais do pipeline."""
        try:
            # Get comprehensive analytics
            scaffold_analytics = await self.bigquery_client.get_scaffold_analytics(time_window_hours=24)
            collaboration_analytics = await self.bigquery_client.get_collaboration_analytics()
            
            final_analytics = {
                "scaffold_analytics": scaffold_analytics,
                "collaboration_analytics": collaboration_analytics,
                "pipeline_performance": {
                    "total_processing_time_hours": self.processing_stats.total_processing_time_ms / (1000 * 3600),
                    "average_speedup_achieved": self.processing_stats.average_speedup_factor,
                    "throughput_achieved": self.processing_stats.throughput_scaffolds_per_second,
                    "peak_memory_usage_gb": self.processing_stats.peak_memory_usage_mb / 1024,
                    "target_speedup_1000x": self.processing_stats.average_speedup_factor >= 1000,
                    "ultra_performance_achieved": self.processing_stats.throughput_scaffolds_per_second >= 100
                },
                "quality_metrics": {
                    "biocompatibility_analysis": scaffold_analytics.get("biocompatibility_distribution", {}),
                    "kec_metrics_distribution": {
                        "avg_h_spectral": scaffold_analytics.get("avg_h_spectral", 0),
                        "avg_k_forman_mean": scaffold_analytics.get("avg_k_forman_mean", 0),
                        "avg_sigma": scaffold_analytics.get("avg_sigma", 0),
                        "avg_swp": scaffold_analytics.get("avg_swp", 0)
                    }
                }
            }
            
            logger.info("üìà Final analytics generated")
            return final_analytics
            
        except Exception as e:
            logger.error(f"Final analytics error: {e}")
            return {"error": str(e)}
    
    async def generate_synthetic_scaffolds(
        self,
        count: int = 10000,
        matrix_sizes: List[int] = [50, 100, 200],
        material_types: List[str] = ["collagen", "chitosan", "PLGA", "hydroxyapatite"]
    ) -> List[ScaffoldInput]:
        """
        üî¨ GERA SCAFFOLDS SINT√âTICOS PARA TESTING
        
        Gera scaffolds sint√©ticos com propriedades diversas para testar pipeline.
        """
        try:
            logger.info(f"üî¨ Generating {count} synthetic scaffolds...")
            
            scaffolds = []
            
            for i in range(count):
                # Random matrix size
                size = np.random.choice(matrix_sizes)
                
                # Generate random adjacency matrix
                # Create sparse matrix with controlled connectivity
                density = np.random.uniform(0.1, 0.4)  # 10-40% connectivity
                matrix = np.random.rand(size, size)
                matrix = (matrix + matrix.T) / 2  # Make symmetric
                matrix = (matrix < density).astype(float)  # Threshold for sparsity
                np.fill_diagonal(matrix, 0)  # Remove self-loops
                
                # Random material and application
                material_type = np.random.choice(material_types)
                applications = ["bone_regeneration", "cartilage_repair", "neural_tissue", "cardiovascular"]
                target_application = np.random.choice(applications)
                
                # Create scaffold input
                scaffold = ScaffoldInput(
                    scaffold_id=f"synthetic_scaffold_{i:06d}",
                    adjacency_matrix=matrix,
                    material_type=material_type,
                    target_application=target_application,
                    metadata={
                        "generation_method": "synthetic",
                        "density": density,
                        "generation_timestamp": datetime.now(timezone.utc).isoformat()
                    }
                )
                
                scaffolds.append(scaffold)
            
            logger.info(f"‚úÖ Generated {len(scaffolds)} synthetic scaffolds")
            return scaffolds
            
        except Exception as e:
            logger.error(f"Synthetic scaffold generation error: {e}")
            return []
    
    async def benchmark_million_scaffold_performance(self) -> Dict[str, Any]:
        """
        üìä BENCHMARK MILLION SCAFFOLD PERFORMANCE
        
        Testa performance do pipeline com datasets crescentes.
        """
        try:
            logger.info("üìä Benchmarking million scaffold performance...")
            
            benchmark_results = {}
            test_sizes = [1000, 5000, 10000, 50000, 100000]
            
            for test_size in test_sizes:
                logger.info(f"üß™ Testing with {test_size} scaffolds...")
                
                # Generate test scaffolds
                test_scaffolds = await self.generate_synthetic_scaffolds(
                    count=test_size,
                    matrix_sizes=[50, 100],  # Smaller matrices for benchmark
                    material_types=["collagen", "chitosan"]
                )
                
                # Process with pipeline
                start_time = time.time()
                
                result = await self.process_million_scaffolds(
                    scaffolds=test_scaffolds,
                    enable_biocompatibility_analysis=True,
                    enable_real_time_analytics=False  # Disable for benchmark
                )
                
                total_time = time.time() - start_time
                
                # Record benchmark results
                benchmark_results[f"size_{test_size}"] = {
                    "scaffold_count": test_size,
                    "total_time_seconds": total_time,
                    "throughput_scaffolds_per_second": result["throughput_scaffolds_per_second"],
                    "average_speedup_factor": result["average_speedup_factor"],
                    "success_rate": result["success_rate"],
                    "memory_peak_gb": self.processing_stats.peak_memory_usage_mb / 1024
                }
                
                logger.info(f"‚úÖ Benchmark {test_size}: {result['throughput_scaffolds_per_second']:.1f} scaffolds/s, {result['average_speedup_factor']:.1f}x speedup")
                
                # Delay between tests
                await asyncio.sleep(5)
            
            # Calculate scaling efficiency
            scaling_analysis = self._analyze_scaling_efficiency(benchmark_results)
            
            final_benchmark = {
                "benchmark_results": benchmark_results,
                "scaling_analysis": scaling_analysis,
                "performance_targets": {
                    "target_throughput": 1000,  # scaffolds/second
                    "target_speedup": 1000,     # 1000x vs baseline
                    "target_success_rate": 0.95  # 95% success
                },
                "benchmark_timestamp": datetime.now(timezone.utc)
            }
            
            logger.info("üìä Million scaffold benchmark completed!")
            return final_benchmark
            
        except Exception as e:
            logger.error(f"Benchmark error: {e}")
            return {"error": str(e)}
    
    def _analyze_scaling_efficiency(self, benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa efici√™ncia de scaling do pipeline."""
        try:
            sizes = []
            throughputs = []
            
            for key, result in benchmark_results.items():
                if key.startswith("size_"):
                    sizes.append(result["scaffold_count"])
                    throughputs.append(result["throughput_scaffolds_per_second"])
            
            if len(sizes) < 2:
                return {"error": "Insufficient data for scaling analysis"}
            
            # Calculate scaling efficiency
            scaling_factors = []
            for i in range(1, len(sizes)):
                size_ratio = sizes[i] / sizes[i-1]
                throughput_ratio = throughputs[i] / throughputs[i-1]
                scaling_efficiency = throughput_ratio / size_ratio
                scaling_factors.append(scaling_efficiency)
            
            avg_scaling_efficiency = np.mean(scaling_factors)
            
            # Determine scaling classification
            if avg_scaling_efficiency >= 0.9:
                scaling_class = "excellent"
            elif avg_scaling_efficiency >= 0.7:
                scaling_class = "good"
            elif avg_scaling_efficiency >= 0.5:
                scaling_class = "moderate"
            else:
                scaling_class = "poor"
            
            return {
                "average_scaling_efficiency": avg_scaling_efficiency,
                "scaling_classification": scaling_class,
                "linear_scaling_achieved": avg_scaling_efficiency >= 0.8,
                "peak_throughput": max(throughputs),
                "optimal_batch_size": sizes[throughputs.index(max(throughputs))],
                "million_scaffold_ready": max(throughputs) >= 100  # 100+ scaffolds/s
            }
            
        except Exception as e:
            logger.error(f"Scaling analysis error: {e}")
            return {"error": str(e)}
    
    async def get_pipeline_status(self) -> Dict[str, Any]:
        """Status completo do pipeline."""
        return {
            "pipeline_initialized": self.is_initialized,
            "currently_processing": self.is_processing,
            "project_id": self.project_id,
            "batch_size": self.batch_size,
            "max_concurrent_batches": self.max_concurrent_batches,
            "processing_stats": asdict(self.processing_stats),
            "jax_engine_ready": self.jax_engine.is_initialized if self.jax_engine else False,
            "bigquery_ready": self.bigquery_client.is_initialized if self.bigquery_client else False,
            "capabilities": [
                "million_scaffold_processing",
                "jax_ultra_performance",
                "bigquery_streaming_pipeline",
                "real_time_analytics",
                "biocompatibility_analysis",
                "performance_benchmarking"
            ]
        }
    
    async def shutdown(self):
        """Shutdown do pipeline."""
        try:
            logger.info("üõë Shutting down Million Scaffold Pipeline...")
            
            # Stop any ongoing processing
            self.is_processing = False
            
            # Shutdown components
            if self.jax_engine:
                await self.jax_engine.shutdown()
            
            if self.bigquery_client:
                await self.bigquery_client.shutdown()
            
            # Shutdown thread pool
            self.executor.shutdown(wait=True)
            
            logger.info("‚úÖ Million Scaffold Pipeline shutdown complete")
            
        except Exception as e:
            logger.error(f"Pipeline shutdown error: {e}")


# ==================== EXPORTS ====================

__all__ = [
    "MillionScaffoldPipeline",
    "ScaffoldInput",
    "PipelineMetrics",
    "ScaffoldAnalysisRecord",
    "ResearchInsightRecord",
    "PerformanceRecord"
]