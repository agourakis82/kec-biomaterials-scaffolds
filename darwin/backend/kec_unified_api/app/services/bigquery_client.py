"""BigQuery Client - Data Pipeline Revolutionary para DARWIN

ðŸ“Š BIGQUERY CLIENT REVOLUTIONARY SYSTEM
Cliente Ã©pico para BigQuery integration com DARWIN:
- Research insights storage and analytics
- Million scaffold results processing
- Performance metrics tracking
- Cross-domain research analytics
- Real-time dashboards data

Technology: BigQuery + Streaming Inserts + Data Pipeline + Analytics
"""

import asyncio
import logging
import uuid
import json
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum

from ..core.logging import get_logger

logger = get_logger("darwin.bigquery_client")

# ImportaÃ§Ãµes condicionais BigQuery
try:
    from google.cloud import bigquery
    from google.cloud.bigquery import Client, Dataset, Table, SchemaField
    from google.cloud.bigquery.enums import SqlTypeNames
    from google.cloud.exceptions import NotFound, Conflict
    from google.auth import default
    BIGQUERY_AVAILABLE = True
    logger.info("ðŸ“Š Google Cloud BigQuery loaded - Data Pipeline Ready!")
except ImportError as e:
    logger.warning(f"BigQuery nÃ£o disponÃ­vel: {e}")
    BIGQUERY_AVAILABLE = False
    bigquery = None
    Client = object


class DatasetType(str, Enum):
    """Tipos de datasets BigQuery."""
    RESEARCH_INSIGHTS = "darwin_research_insights"
    PERFORMANCE_METRICS = "darwin_performance_metrics"
    SCAFFOLD_RESULTS = "darwin_scaffold_results"
    TRAINING_LOGS = "darwin_training_logs"
    COLLABORATION_DATA = "darwin_collaboration_data"
    REAL_TIME_ANALYTICS = "darwin_real_time_analytics"


@dataclass
class ResearchInsightRecord:
    """Record para insights de pesquisa."""
    insight_id: str
    research_id: str
    agent_specialization: str
    insight_content: str
    confidence_score: float
    insight_type: str
    evidence_sources: List[str]
    timestamp: datetime
    domain_tags: List[str]
    collaboration_context: Optional[str] = None
    cross_domain_connections: Optional[List[str]] = None


@dataclass
class ScaffoldAnalysisRecord:
    """Record para resultados de anÃ¡lise de scaffold."""
    scaffold_id: str
    analysis_id: str
    kec_metrics: Dict[str, float]  # H_spectral, k_forman_mean, sigma, swp
    material_properties: Dict[str, Any]
    biocompatibility_score: float
    performance_metrics: Dict[str, float]
    computation_time_ms: float
    jax_speedup_factor: float
    timestamp: datetime
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class CollaborationRecord:
    """Record para colaboraÃ§Ãµes entre agents."""
    collaboration_id: str
    research_question: str
    participating_agents: List[str]
    insights_generated: int
    collaboration_duration_ms: float
    success_score: float
    interdisciplinary_connections: int
    novel_insights_count: int
    timestamp: datetime


@dataclass
class PerformanceRecord:
    """Record para mÃ©tricas de performance."""
    session_id: str
    component: str  # jax_engine, autogen_team, vertex_ai, etc.
    operation: str
    duration_ms: float
    throughput: float
    memory_usage_mb: float
    cpu_usage_percent: float
    gpu_usage_percent: Optional[float]
    success: bool
    error_message: Optional[str]
    timestamp: datetime


class BigQueryClient:
    """
    ðŸ“Š BIGQUERY CLIENT REVOLUTIONARY
    
    Cliente completo para BigQuery com:
    - Research insights storage para colaboraÃ§Ãµes IA
    - Million scaffold results pipeline
    - Performance metrics tracking
    - Cross-domain analytics
    - Real-time dashboard data
    """
    
    def __init__(self, project_id: str, location: str = "us-central1"):
        self.project_id = project_id
        self.location = location
        self.is_initialized = False
        
        # BigQuery client
        self.client: Optional[Client] = None
        
        # Dataset references
        self.datasets: Dict[str, Dataset] = {}
        self.tables: Dict[str, Table] = {}
        
        # Streaming configuration
        self.streaming_config = {
            "max_batch_size": 1000,
            "max_latency_ms": 1000,
            "auto_create_tables": True
        }
        
        # Data pipeline stats
        self.pipeline_stats = {
            "total_records_inserted": 0,
            "total_batches_processed": 0,
            "average_latency_ms": 0.0,
            "error_count": 0
        }
        
        logger.info(f"ðŸ“Š BigQuery Client created: {project_id}")
    
    async def initialize(self):
        """Inicializa BigQuery client."""
        try:
            logger.info("ðŸ“Š Inicializando BigQuery Client...")
            
            if not BIGQUERY_AVAILABLE:
                logger.warning("BigQuery nÃ£o disponÃ­vel - funcionando em modo simulaÃ§Ã£o")
                self.is_initialized = True
                return
            
            # Initialize BigQuery client
            self.client = bigquery.Client(project=self.project_id)
            
            # Create datasets se nÃ£o existirem
            await self._create_datasets()
            
            # Create tables se nÃ£o existirem
            await self._create_tables()
            
            # Verificar conectividade
            await self._verify_connectivity()
            
            self.is_initialized = True
            logger.info("âœ… BigQuery Client initialized successfully!")
            
        except Exception as e:
            logger.error(f"Falha na inicializaÃ§Ã£o BigQuery Client: {e}")
            raise
    
    async def _create_datasets(self):
        """Cria datasets BigQuery necessÃ¡rios."""
        try:
            for dataset_type in DatasetType:
                dataset_id = f"{self.project_id}.{dataset_type.value}"
                
                try:
                    # Try to get existing dataset
                    dataset = self.client.get_dataset(dataset_id)
                    logger.info(f"âœ… Dataset exists: {dataset_type.value}")
                    
                except NotFound:
                    # Create new dataset
                    dataset = bigquery.Dataset(dataset_id)
                    dataset.location = self.location
                    dataset.description = f"DARWIN {dataset_type.value.replace('_', ' ').title()} Dataset"
                    
                    dataset = self.client.create_dataset(dataset, timeout=30)
                    logger.info(f"ðŸ“Š Created dataset: {dataset_type.value}")
                
                self.datasets[dataset_type.value] = dataset
            
            logger.info(f"âœ… All datasets ready: {len(self.datasets)} datasets")
            
        except Exception as e:
            logger.error(f"Dataset creation error: {e}")
            raise
    
    async def _create_tables(self):
        """Cria tables com schemas especÃ­ficos."""
        try:
            # Research Insights Table
            await self._create_research_insights_table()
            
            # Scaffold Results Table
            await self._create_scaffold_results_table()
            
            # Collaboration Data Table
            await self._create_collaboration_table()
            
            # Performance Metrics Table
            await self._create_performance_table()
            
            logger.info(f"âœ… All tables created: {len(self.tables)} tables")
            
        except Exception as e:
            logger.error(f"Table creation error: {e}")
            raise
    
    async def _create_research_insights_table(self):
        """Cria table para research insights."""
        schema = [
            SchemaField("insight_id", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("research_id", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("agent_specialization", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("insight_content", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("confidence_score", SqlTypeNames.FLOAT64, mode="REQUIRED"),
            SchemaField("insight_type", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("evidence_sources", SqlTypeNames.STRING, mode="REPEATED"),
            SchemaField("timestamp", SqlTypeNames.TIMESTAMP, mode="REQUIRED"),
            SchemaField("domain_tags", SqlTypeNames.STRING, mode="REPEATED"),
            SchemaField("collaboration_context", SqlTypeNames.STRING, mode="NULLABLE"),
            SchemaField("cross_domain_connections", SqlTypeNames.STRING, mode="REPEATED"),
        ]
        
        await self._create_table_with_schema(
            dataset_name=DatasetType.RESEARCH_INSIGHTS.value,
            table_name="insights",
            schema=schema,
            description="DARWIN Research Team Insights - AutoGen Collaborative Intelligence"
        )
    
    async def _create_scaffold_results_table(self):
        """Cria table para resultados de scaffold analysis."""
        schema = [
            SchemaField("scaffold_id", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("analysis_id", SqlTypeNames.STRING, mode="REQUIRED"),
            # KEC Metrics
            SchemaField("h_spectral", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("k_forman_mean", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("sigma", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("swp", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            # Material Properties (JSON)
            SchemaField("material_properties", SqlTypeNames.JSON, mode="NULLABLE"),
            SchemaField("biocompatibility_score", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            # Performance Metrics
            SchemaField("computation_time_ms", SqlTypeNames.FLOAT64, mode="REQUIRED"),
            SchemaField("jax_speedup_factor", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("memory_usage_mb", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("gpu_utilization", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            # Metadata
            SchemaField("timestamp", SqlTypeNames.TIMESTAMP, mode="REQUIRED"),
            SchemaField("metadata", SqlTypeNames.JSON, mode="NULLABLE"),
        ]
        
        await self._create_table_with_schema(
            dataset_name=DatasetType.SCAFFOLD_RESULTS.value,
            table_name="scaffold_analysis",
            schema=schema,
            description="DARWIN Scaffold Analysis Results - Million Scaffold Processing Pipeline"
        )
    
    async def _create_collaboration_table(self):
        """Cria table para dados de colaboraÃ§Ã£o."""
        schema = [
            SchemaField("collaboration_id", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("research_question", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("participating_agents", SqlTypeNames.STRING, mode="REPEATED"),
            SchemaField("insights_generated", SqlTypeNames.INT64, mode="REQUIRED"),
            SchemaField("collaboration_duration_ms", SqlTypeNames.FLOAT64, mode="REQUIRED"),
            SchemaField("success_score", SqlTypeNames.FLOAT64, mode="REQUIRED"),
            SchemaField("interdisciplinary_connections", SqlTypeNames.INT64, mode="REQUIRED"),
            SchemaField("novel_insights_count", SqlTypeNames.INT64, mode="REQUIRED"),
            SchemaField("domain_coverage", SqlTypeNames.STRING, mode="REPEATED"),
            SchemaField("timestamp", SqlTypeNames.TIMESTAMP, mode="REQUIRED"),
        ]
        
        await self._create_table_with_schema(
            dataset_name=DatasetType.COLLABORATION_DATA.value,
            table_name="collaborations",
            schema=schema,
            description="DARWIN Agent Collaborations - AutoGen Team Performance Analytics"
        )
    
    async def _create_performance_table(self):
        """Cria table para mÃ©tricas de performance."""
        schema = [
            SchemaField("session_id", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("component", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("operation", SqlTypeNames.STRING, mode="REQUIRED"),
            SchemaField("duration_ms", SqlTypeNames.FLOAT64, mode="REQUIRED"),
            SchemaField("throughput", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("memory_usage_mb", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("cpu_usage_percent", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("gpu_usage_percent", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("success", SqlTypeNames.BOOLEAN, mode="REQUIRED"),
            SchemaField("error_message", SqlTypeNames.STRING, mode="NULLABLE"),
            SchemaField("jax_compilation_time", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("speedup_factor", SqlTypeNames.FLOAT64, mode="NULLABLE"),
            SchemaField("timestamp", SqlTypeNames.TIMESTAMP, mode="REQUIRED"),
        ]
        
        await self._create_table_with_schema(
            dataset_name=DatasetType.PERFORMANCE_METRICS.value,
            table_name="performance_metrics",
            schema=schema,
            description="DARWIN Performance Metrics - JAX Ultra-Performance Tracking"
        )
    
    async def _create_table_with_schema(
        self,
        dataset_name: str,
        table_name: str,
        schema: List[SchemaField],
        description: str
    ):
        """Cria table com schema especÃ­fico."""
        try:
            table_id = f"{self.project_id}.{dataset_name}.{table_name}"
            
            try:
                # Check if table exists
                table = self.client.get_table(table_id)
                logger.info(f"âœ… Table exists: {table_name}")
                
            except NotFound:
                # Create new table
                table = bigquery.Table(table_id, schema=schema)
                table.description = description
                
                # Configure partitioning by timestamp for performance
                table.time_partitioning = bigquery.TimePartitioning(
                    type_=bigquery.TimePartitioningType.DAY,
                    field="timestamp"
                )
                
                # Configure clustering for query optimization
                if table_name == "scaffold_analysis":
                    table.clustering_fields = ["scaffold_id", "analysis_id"]
                elif table_name == "insights":
                    table.clustering_fields = ["agent_specialization", "research_id"]
                elif table_name == "collaborations":
                    table.clustering_fields = ["collaboration_id"]
                elif table_name == "performance_metrics":
                    table.clustering_fields = ["component", "operation"]
                
                table = self.client.create_table(table, timeout=30)
                logger.info(f"ðŸ“Š Created table: {table_name}")
            
            self.tables[f"{dataset_name}.{table_name}"] = table
            
        except Exception as e:
            logger.error(f"Table creation error for {table_name}: {e}")
            raise
    
    async def insert_research_insight(self, insight: ResearchInsightRecord) -> bool:
        """Insere insight de pesquisa no BigQuery."""
        try:
            if not BIGQUERY_AVAILABLE:
                logger.info(f"Mock insert research insight: {insight.insight_id}")
                return True
            
            table_id = f"{DatasetType.RESEARCH_INSIGHTS.value}.insights"
            table = self.tables.get(table_id)
            
            if not table:
                logger.error(f"Table not found: {table_id}")
                return False
            
            # Convert to BigQuery row
            row = {
                "insight_id": insight.insight_id,
                "research_id": insight.research_id,
                "agent_specialization": insight.agent_specialization,
                "insight_content": insight.insight_content,
                "confidence_score": insight.confidence_score,
                "insight_type": insight.insight_type,
                "evidence_sources": insight.evidence_sources,
                "timestamp": insight.timestamp,
                "domain_tags": insight.domain_tags,
                "collaboration_context": insight.collaboration_context,
                "cross_domain_connections": insight.cross_domain_connections or []
            }
            
            # Insert row
            errors = self.client.insert_rows_json(table, [row])
            
            if errors:
                logger.error(f"BigQuery insert errors: {errors}")
                return False
            
            logger.info(f"âœ… Research insight inserted: {insight.insight_id}")
            self._update_pipeline_stats(1, True)
            return True
            
        except Exception as e:
            logger.error(f"Research insight insert error: {e}")
            self._update_pipeline_stats(1, False)
            return False
    
    async def insert_scaffold_result(self, scaffold: ScaffoldAnalysisRecord) -> bool:
        """Insere resultado de anÃ¡lise de scaffold."""
        try:
            if not BIGQUERY_AVAILABLE:
                logger.info(f"Mock insert scaffold result: {scaffold.scaffold_id}")
                return True
            
            table_id = f"{DatasetType.SCAFFOLD_RESULTS.value}.scaffold_analysis"
            table = self.tables.get(table_id)
            
            if not table:
                logger.error(f"Table not found: {table_id}")
                return False
            
            # Convert to BigQuery row
            row = {
                "scaffold_id": scaffold.scaffold_id,
                "analysis_id": scaffold.analysis_id,
                "h_spectral": scaffold.kec_metrics.get("H_spectral"),
                "k_forman_mean": scaffold.kec_metrics.get("k_forman_mean"),
                "sigma": scaffold.kec_metrics.get("sigma"),
                "swp": scaffold.kec_metrics.get("swp"),
                "material_properties": scaffold.material_properties,
                "biocompatibility_score": scaffold.biocompatibility_score,
                "computation_time_ms": scaffold.computation_time_ms,
                "jax_speedup_factor": scaffold.jax_speedup_factor,
                "timestamp": scaffold.timestamp,
                "metadata": scaffold.metadata
            }
            
            # Insert row
            errors = self.client.insert_rows_json(table, [row])
            
            if errors:
                logger.error(f"BigQuery insert errors: {errors}")
                return False
            
            logger.info(f"âœ… Scaffold result inserted: {scaffold.scaffold_id}")
            self._update_pipeline_stats(1, True)
            return True
            
        except Exception as e:
            logger.error(f"Scaffold result insert error: {e}")
            self._update_pipeline_stats(1, False)
            return False
    
    async def insert_collaboration_data(self, collaboration: CollaborationRecord) -> bool:
        """Insere dados de colaboraÃ§Ã£o entre agents."""
        try:
            if not BIGQUERY_AVAILABLE:
                logger.info(f"Mock insert collaboration: {collaboration.collaboration_id}")
                return True
            
            table_id = f"{DatasetType.COLLABORATION_DATA.value}.collaborations"
            table = self.tables.get(table_id)
            
            if not table:
                logger.error(f"Table not found: {table_id}")
                return False
            
            # Convert to BigQuery row
            row = {
                "collaboration_id": collaboration.collaboration_id,
                "research_question": collaboration.research_question,
                "participating_agents": collaboration.participating_agents,
                "insights_generated": collaboration.insights_generated,
                "collaboration_duration_ms": collaboration.collaboration_duration_ms,
                "success_score": collaboration.success_score,
                "interdisciplinary_connections": collaboration.interdisciplinary_connections,
                "novel_insights_count": collaboration.novel_insights_count,
                "domain_coverage": collaboration.participating_agents,  # Use agents as domain coverage
                "timestamp": collaboration.timestamp
            }
            
            # Insert row
            errors = self.client.insert_rows_json(table, [row])
            
            if errors:
                logger.error(f"BigQuery insert errors: {errors}")
                return False
            
            logger.info(f"âœ… Collaboration data inserted: {collaboration.collaboration_id}")
            self._update_pipeline_stats(1, True)
            return True
            
        except Exception as e:
            logger.error(f"Collaboration insert error: {e}")
            self._update_pipeline_stats(1, False)
            return False
    
    async def insert_performance_metrics(self, performance: PerformanceRecord) -> bool:
        """Insere mÃ©tricas de performance."""
        try:
            if not BIGQUERY_AVAILABLE:
                logger.info(f"Mock insert performance: {performance.session_id}")
                return True
            
            table_id = f"{DatasetType.PERFORMANCE_METRICS.value}.performance_metrics"
            table = self.tables.get(table_id)
            
            if not table:
                logger.error(f"Table not found: {table_id}")
                return False
            
            # Convert to BigQuery row
            row = asdict(performance)
            
            # Insert row
            errors = self.client.insert_rows_json(table, [row])
            
            if errors:
                logger.error(f"BigQuery insert errors: {errors}")
                return False
            
            logger.info(f"âœ… Performance metrics inserted: {performance.session_id}")
            self._update_pipeline_stats(1, True)
            return True
            
        except Exception as e:
            logger.error(f"Performance metrics insert error: {e}")
            self._update_pipeline_stats(1, False)
            return False
    
    async def batch_insert_scaffold_results(
        self,
        scaffolds: List[ScaffoldAnalysisRecord],
        batch_size: int = 1000
    ) -> Dict[str, Any]:
        """
        ðŸŒŠ BATCH INSERT MILLION SCAFFOLD RESULTS
        
        Insere milhÃµes de resultados de scaffold em lotes otimizados.
        """
        try:
            logger.info(f"ðŸŒŠ Batch inserting {len(scaffolds)} scaffold results...")
            
            if not BIGQUERY_AVAILABLE:
                logger.info("Mock batch insert completed")
                return {
                    "total_inserted": len(scaffolds),
                    "batches_processed": len(scaffolds) // batch_size + 1,
                    "success_rate": 1.0,
                    "errors": []
                }
            
            table_id = f"{DatasetType.SCAFFOLD_RESULTS.value}.scaffold_analysis"
            table = self.tables.get(table_id)
            
            if not table:
                raise RuntimeError(f"Table not found: {table_id}")
            
            # Process in batches
            total_inserted = 0
            total_batches = 0
            errors_list = []
            
            for i in range(0, len(scaffolds), batch_size):
                batch = scaffolds[i:i + batch_size]
                total_batches += 1
                
                # Convert batch to BigQuery rows
                rows = []
                for scaffold in batch:
                    row = {
                        "scaffold_id": scaffold.scaffold_id,
                        "analysis_id": scaffold.analysis_id,
                        "h_spectral": scaffold.kec_metrics.get("H_spectral"),
                        "k_forman_mean": scaffold.kec_metrics.get("k_forman_mean"),
                        "sigma": scaffold.kec_metrics.get("sigma"),
                        "swp": scaffold.kec_metrics.get("swp"),
                        "material_properties": scaffold.material_properties,
                        "biocompatibility_score": scaffold.biocompatibility_score,
                        "computation_time_ms": scaffold.computation_time_ms,
                        "jax_speedup_factor": scaffold.jax_speedup_factor,
                        "timestamp": scaffold.timestamp,
                        "metadata": scaffold.metadata
                    }
                    rows.append(row)
                
                # Insert batch
                batch_errors = self.client.insert_rows_json(table, rows)
                
                if batch_errors:
                    errors_list.extend(batch_errors)
                    logger.warning(f"Batch {total_batches} had {len(batch_errors)} errors")
                else:
                    total_inserted += len(batch)
                    logger.info(f"âœ… Batch {total_batches} inserted: {len(batch)} scaffolds")
                
                # Small delay to avoid rate limits
                await asyncio.sleep(0.1)
            
            success_rate = total_inserted / len(scaffolds) if scaffolds else 0.0
            
            result = {
                "total_inserted": total_inserted,
                "total_requested": len(scaffolds),
                "batches_processed": total_batches,
                "success_rate": success_rate,
                "errors": errors_list[:10]  # Limit error list
            }
            
            logger.info(f"ðŸŒŠ Batch insert completed: {total_inserted}/{len(scaffolds)} success ({success_rate*100:.1f}%)")
            self._update_pipeline_stats(total_inserted, success_rate > 0.9)
            
            return result
            
        except Exception as e:
            logger.error(f"Batch insert error: {e}")
            raise
    
    async def query_research_insights(
        self,
        agent_specialization: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Query research insights com filtros."""
        try:
            if not BIGQUERY_AVAILABLE:
                # Mock results
                return [
                    {
                        "insight_id": "mock-insight-1",
                        "agent_specialization": agent_specialization or "biomaterials",
                        "insight_content": "Mock research insight for testing",
                        "confidence_score": 0.85,
                        "timestamp": datetime.now(timezone.utc)
                    }
                ]
            
            # Build query
            query = f"""
            SELECT 
                insight_id,
                research_id,
                agent_specialization,
                insight_content,
                confidence_score,
                insight_type,
                evidence_sources,
                timestamp,
                domain_tags
            FROM `{self.project_id}.{DatasetType.RESEARCH_INSIGHTS.value}.insights`
            WHERE 1=1
            """
            
            if agent_specialization:
                query += f" AND agent_specialization = '{agent_specialization}'"
            
            if start_date:
                query += f" AND timestamp >= '{start_date.isoformat()}'"
            
            if end_date:
                query += f" AND timestamp <= '{end_date.isoformat()}'"
            
            query += f" ORDER BY timestamp DESC LIMIT {limit}"
            
            # Execute query
            query_job = self.client.query(query)
            results = query_job.result()
            
            # Convert to list of dicts
            insights = []
            for row in results:
                insights.append(dict(row))
            
            logger.info(f"ðŸ“Š Queried {len(insights)} research insights")
            return insights
            
        except Exception as e:
            logger.error(f"Query research insights error: {e}")
            return []
    
    async def get_scaffold_analytics(
        self,
        time_window_hours: int = 24
    ) -> Dict[str, Any]:
        """
        ðŸ“ˆ ANALYTICS DE SCAFFOLD - MILLION SCAFFOLD INSIGHTS
        
        Retorna analytics avanÃ§ados dos scaffolds processados.
        """
        try:
            if not BIGQUERY_AVAILABLE:
                # Mock analytics
                return {
                    "total_scaffolds_analyzed": 125000,
                    "avg_h_spectral": 7.2,
                    "avg_k_forman_mean": 0.31,
                    "avg_sigma": 2.1,
                    "avg_swp": 0.78,
                    "avg_computation_time_ms": 15.3,
                    "avg_speedup_factor": 847.2,
                    "biocompatibility_distribution": {"high": 45, "medium": 40, "low": 15},
                    "time_window_hours": time_window_hours
                }
            
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(hours=time_window_hours)
            
            # Analytics query
            query = f"""
            SELECT 
                COUNT(*) as total_scaffolds,
                AVG(h_spectral) as avg_h_spectral,
                AVG(k_forman_mean) as avg_k_forman_mean,
                AVG(sigma) as avg_sigma,
                AVG(swp) as avg_swp,
                AVG(computation_time_ms) as avg_computation_time,
                AVG(jax_speedup_factor) as avg_speedup_factor,
                STDDEV(h_spectral) as std_h_spectral,
                MIN(computation_time_ms) as min_computation_time,
                MAX(computation_time_ms) as max_computation_time,
                COUNT(CASE WHEN biocompatibility_score > 0.8 THEN 1 END) as high_biocompatibility,
                COUNT(CASE WHEN biocompatibility_score BETWEEN 0.5 AND 0.8 THEN 1 END) as medium_biocompatibility,
                COUNT(CASE WHEN biocompatibility_score < 0.5 THEN 1 END) as low_biocompatibility
            FROM `{self.project_id}.{DatasetType.SCAFFOLD_RESULTS.value}.scaffold_analysis`
            WHERE timestamp >= '{start_time.isoformat()}'
              AND timestamp <= '{end_time.isoformat()}'
            """
            
            # Execute analytics query
            query_job = self.client.query(query)
            results = list(query_job.result())
            
            if results:
                row = results[0]
                analytics = {
                    "total_scaffolds_analyzed": row.total_scaffolds or 0,
                    "avg_h_spectral": round(row.avg_h_spectral or 0, 3),
                    "avg_k_forman_mean": round(row.avg_k_forman_mean or 0, 3),
                    "avg_sigma": round(row.avg_sigma or 0, 3),
                    "avg_swp": round(row.avg_swp or 0, 3),
                    "avg_computation_time_ms": round(row.avg_computation_time or 0, 2),
                    "avg_speedup_factor": round(row.avg_speedup_factor or 0, 1),
                    "std_h_spectral": round(row.std_h_spectral or 0, 3),
                    "min_computation_time_ms": row.min_computation_time or 0,
                    "max_computation_time_ms": row.max_computation_time or 0,
                    "biocompatibility_distribution": {
                        "high": row.high_biocompatibility or 0,
                        "medium": row.medium_biocompatibility or 0,
                        "low": row.low_biocompatibility or 0
                    },
                    "time_window_hours": time_window_hours,
                    "query_timestamp": datetime.now(timezone.utc)
                }
            else:
                analytics = {"error": "No data found", "time_window_hours": time_window_hours}
            
            logger.info(f"ðŸ“ˆ Scaffold analytics generated: {analytics.get('total_scaffolds_analyzed', 0)} scaffolds")
            return analytics
            
        except Exception as e:
            logger.error(f"Scaffold analytics error: {e}")
            return {"error": str(e)}
    
    async def get_collaboration_analytics(self) -> Dict[str, Any]:
        """Analytics das colaboraÃ§Ãµes entre agents."""
        try:
            if not BIGQUERY_AVAILABLE:
                return {
                    "total_collaborations": 1250,
                    "avg_success_score": 0.87,
                    "avg_insights_per_collaboration": 4.2,
                    "most_active_agent": "Dr_Biomaterials",
                    "interdisciplinary_rate": 0.78
                }
            
            # Collaboration analytics query
            query = f"""
            SELECT 
                COUNT(*) as total_collaborations,
                AVG(success_score) as avg_success_score,
                AVG(insights_generated) as avg_insights_generated,
                AVG(collaboration_duration_ms) as avg_duration_ms,
                AVG(interdisciplinary_connections) as avg_interdisciplinary_connections,
                MAX(insights_generated) as max_insights_single_collaboration
            FROM `{self.project_id}.{DatasetType.COLLABORATION_DATA.value}.collaborations`
            WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
            """
            
            # Execute query
            query_job = self.client.query(query)
            results = list(query_job.result())
            
            if results:
                row = results[0]
                analytics = {
                    "total_collaborations": row.total_collaborations or 0,
                    "avg_success_score": round(row.avg_success_score or 0, 3),
                    "avg_insights_per_collaboration": round(row.avg_insights_generated or 0, 2),
                    "avg_duration_ms": round(row.avg_duration_ms or 0, 1),
                    "avg_interdisciplinary_connections": round(row.avg_interdisciplinary_connections or 0, 2),
                    "max_insights_single_collaboration": row.max_insights_single_collaboration or 0,
                    "analysis_period": "last_7_days"
                }
            else:
                analytics = {"error": "No collaboration data found"}
            
            return analytics
            
        except Exception as e:
            logger.error(f"Collaboration analytics error: {e}")
            return {"error": str(e)}
    
    def _update_pipeline_stats(self, records_count: int, success: bool):
        """Atualiza estatÃ­sticas do pipeline."""
        self.pipeline_stats["total_records_inserted"] += records_count if success else 0
        self.pipeline_stats["total_batches_processed"] += 1
        
        if not success:
            self.pipeline_stats["error_count"] += 1
    
    async def _verify_connectivity(self):
        """Verifica conectividade BigQuery."""
        try:
            # Simple query para testar conectividade
            query = f"SELECT CURRENT_TIMESTAMP() as current_time"
            query_job = self.client.query(query)
            result = list(query_job.result())[0]
            
            logger.info(f"âœ… BigQuery connectivity verified: {result.current_time}")
            
        except Exception as e:
            logger.warning(f"BigQuery connectivity test failed: {e}")
    
    async def get_client_status(self) -> Dict[str, Any]:
        """Status completo do BigQuery client."""
        return {
            "client_initialized": self.is_initialized,
            "bigquery_available": BIGQUERY_AVAILABLE,
            "project_id": self.project_id,
            "location": self.location,
            "datasets_count": len(self.datasets),
            "tables_count": len(self.tables),
            "pipeline_stats": self.pipeline_stats.copy(),
            "capabilities": [
                "research_insights_storage",
                "million_scaffold_processing",
                "collaboration_analytics",
                "performance_tracking",
                "real_time_dashboards",
                "cross_domain_analytics"
            ]
        }
    
    async def shutdown(self):
        """Shutdown do BigQuery client."""
        try:
            logger.info("ðŸ›‘ Shutting down BigQuery Client...")
            
            # Close client connections
            if self.client:
                self.client.close()
            
            # Clear references
            self.datasets.clear()
            self.tables.clear()
            
            self.is_initialized = False
            logger.info("âœ… BigQuery Client shutdown complete")
            
        except Exception as e:
            logger.error(f"BigQuery shutdown error: {e}")


# ==================== EXPORTS ====================

__all__ = [
    "BigQueryClient",
    "DatasetType",
    "ResearchInsightRecord",
    "ScaffoldAnalysisRecord", 
    "CollaborationRecord",
    "PerformanceRecord",
    "BIGQUERY_AVAILABLE"
]